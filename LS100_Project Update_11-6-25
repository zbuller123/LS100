{"cells":[{"cell_type":"markdown","source":["First, you need to mount the Google Drive in which you have the videos, and navigate to the directory where the videos are saved."],"metadata":{"id":"1YP7jWm_6_jf"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"ZwQwazmIYVsZ","executionInfo":{"status":"ok","timestamp":1762813560702,"user_tz":300,"elapsed":15986,"user":{"displayName":"Souvik Mandal","userId":"02704498209737744010"}},"outputId":"f052e9d9-53c0-48cd-d6a1-745894022940","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/LS100_2025"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IDbWHjTIayqQ","executionInfo":{"status":"ok","timestamp":1762813574833,"user_tz":300,"elapsed":14,"user":{"displayName":"Souvik Mandal","userId":"02704498209737744010"}},"outputId":"1afc9814-d596-4b12-dd5a-cdb704500b35"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/LS100_2025\n"]}]},{"cell_type":"code","source":["%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E7tHhYi7bX28","executionInfo":{"status":"ok","timestamp":1762813577446,"user_tz":300,"elapsed":107,"user":{"displayName":"Souvik Mandal","userId":"02704498209737744010"}},"outputId":"38b07792-3347-424f-a804-77d8f68223fd"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["sprintblockstart_221008_clip015_isaiah_crpd.mp4\n"]}]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hnPy1tK148au","executionInfo":{"status":"ok","timestamp":1762813798113,"user_tz":300,"elapsed":123,"user":{"displayName":"Souvik Mandal","userId":"02704498209737744010"}},"outputId":"5217d5ca-0c87-4296-fccb-409b2563a5e8"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Nov 10 22:29:58 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   44C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["import torch\n","print(\"torch cuda:\", torch.cuda.is_available(), torch.cuda.get_device_name(0) if torch.cuda.is_available() else None)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7EHw23tE6NoH","executionInfo":{"status":"ok","timestamp":1762814129484,"user_tz":300,"elapsed":4219,"user":{"displayName":"Souvik Mandal","userId":"02704498209737744010"}},"outputId":"057eb67c-1ada-47f7-f313-253161d85b99"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["torch cuda: True Tesla T4\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","print(\"tf gpus:\", tf.config.list_physical_devices(\"GPU\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uByZnk0H6WON","executionInfo":{"status":"ok","timestamp":1762814171767,"user_tz":300,"elapsed":10153,"user":{"displayName":"Souvik Mandal","userId":"02704498209737744010"}},"outputId":"200b3ff5-5ea2-4b43-d372-4297f24d7b22"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["tf gpus: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"HC56im5XbdXy"}},{"cell_type":"code","source":["# ============================================\n","# 0. Environment Setup and Package Verification\n","# ============================================\n","\n","import sys\n","import importlib\n","import subprocess\n","\n","# ---- 1. Check Python version ----\n","py_version = sys.version_info\n","print(f\"üß† Python version: {py_version.major}.{py_version.minor}.{py_version.micro}\")\n","if py_version < (3, 9) or py_version >= (3, 13):\n","    print(\"‚ö†Ô∏è MediaPipe Tasks officially supports Python 3.9‚Äì3.12.\")\n","    print(\"‚ö†Ô∏è Please switch to Python 3.12 for this notebook (as used in LS100).\")\n","\n","# ---- 2. Define required packages ----\n","required_packages = [\n","    \"mediapipe\",\n","    \"opencv-python\",\n","    \"pandas\",\n","    \"numpy\",\n","    \"tqdm\",\n","    \"matplotlib\",\n","    \"seaborn\",\n","]\n","\n","# ---- 3. Function to check and install ----\n","def install_if_missing(pkg):\n","    \"\"\"\n","    Try importing the package; if not found, install it quietly.\n","    \"\"\"\n","    try:\n","        importlib.import_module(pkg.split(\"==\")[0])\n","        print(f\"‚úÖ {pkg} already installed\")\n","    except ImportError:\n","        print(f\"‚¨áÔ∏è Installing {pkg} ...\")\n","        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n","\n","# ---- 4. Verify each dependency ----\n","for package in required_packages:\n","    install_if_missing(package)\n","\n","# ---- 5. Print package versions for reproducibility ----\n","import mediapipe as mp\n","import cv2, pandas as pd, numpy as np, tqdm, matplotlib, seaborn\n","\n","print(\"\\nüì¶ Package versions:\")\n","print(f\"mediapipe      : {mp.__version__}\")\n","print(f\"opencv-python  : {cv2.__version__}\")\n","print(f\"pandas         : {pd.__version__}\")\n","print(f\"numpy          : {np.__version__}\")\n","print(f\"matplotlib     : {matplotlib.__version__}\")\n","print(f\"seaborn        : {seaborn.__version__}\")\n","\n","print(\"\\n‚úÖ Environment is ready to proceed!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DdfASyeNaOWM","executionInfo":{"status":"ok","timestamp":1762814512251,"user_tz":300,"elapsed":33304,"user":{"displayName":"Souvik Mandal","userId":"02704498209737744010"}},"outputId":"feca9970-0e18-481c-8179-902fedbb7c9a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["üß† Python version: 3.12.12\n","‚¨áÔ∏è Installing mediapipe ...\n","‚¨áÔ∏è Installing opencv-python ...\n","‚úÖ pandas already installed\n","‚úÖ numpy already installed\n","‚úÖ tqdm already installed\n","‚úÖ matplotlib already installed\n","‚úÖ seaborn already installed\n","\n","üì¶ Package versions:\n","mediapipe      : 0.10.21\n","opencv-python  : 4.12.0\n","pandas         : 2.2.2\n","numpy          : 2.0.2\n","matplotlib     : 3.10.0\n","seaborn        : 0.13.2\n","\n","‚úÖ Environment is ready to proceed!\n"]}]},{"cell_type":"code","source":["# ======================================\n","# 1. Import Libraries and Verify Versions (fixed for MediaPipe >=0.10)\n","# ======================================\n","\n","import os, cv2, numpy as np, pandas as pd, matplotlib, seaborn as sns\n","from tqdm import tqdm\n","\n","import mediapipe as mp\n","from mediapipe.tasks import python as mp_python\n","from mediapipe.tasks.python import vision as mp_vision\n","\n","print(\"‚úÖ MediaPipe Tasks API imported successfully!\\n\")\n","print(f\"mediapipe version : {mp.__version__}\")\n","print(f\"opencv version    : {cv2.__version__}\")\n","print(f\"pandas version    : {pd.__version__}\")\n","print(f\"numpy version     : {np.__version__}\")\n","\n","# Optional: check GPU availability\n","backend = \"GPU\" if cv2.cuda.getCudaEnabledDeviceCount() > 0 else \"CPU\"\n","print(f\"‚öôÔ∏è Running on {backend}\")\n","\n","# ---- Smoke test: confirm Tasks API symbols exist ----\n","BaseOptions = mp_python.BaseOptions\n","PoseLandmarker = mp_vision.PoseLandmarker\n","PoseLandmarkerOptions = mp_vision.PoseLandmarkerOptions\n","RunningMode = mp_vision.RunningMode\n","\n","print(\"\\n MediaPipe Tasks API is available:\")\n","print(f\"- BaseOptions           : {BaseOptions is not None}\")\n","print(f\"- PoseLandmarker        : {PoseLandmarker is not None}\")\n","print(f\"- PoseLandmarkerOptions : {PoseLandmarkerOptions is not None}\")\n","print(f\"- RunningMode           : {RunningMode is not None}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"13ioSTkkcYx3","executionInfo":{"status":"ok","timestamp":1762814520981,"user_tz":300,"elapsed":5,"user":{"displayName":"Souvik Mandal","userId":"02704498209737744010"}},"outputId":"f8557d05-aef7-434a-baa5-89e0261489a7"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ MediaPipe Tasks API imported successfully!\n","\n","mediapipe version : 0.10.21\n","opencv version    : 4.12.0\n","pandas version    : 2.2.2\n","numpy version     : 2.0.2\n","‚öôÔ∏è Running on CPU\n","\n"," MediaPipe Tasks API is available:\n","- BaseOptions           : True\n","- PoseLandmarker        : True\n","- PoseLandmarkerOptions : True\n","- RunningMode           : True\n"]}]},{"cell_type":"code","source":["# ================================\n","# 2. Model Selection & Download\n","# ================================\n","import os\n","import pathlib\n","import urllib.request\n","import urllib.error\n","import time\n","\n","import mediapipe as mp\n","from mediapipe.tasks import python as mp_python\n","from mediapipe.tasks.python import vision as mp_vision\n","\n","# ---- Where to save models ----\n","MODELS_DIR = pathlib.Path(\"models\")\n","MODELS_DIR.mkdir(parents=True, exist_ok=True)\n","\n","# ---- Official model URLs (float16 is ideal for GPU/T4) ----\n","MODEL_URLS = {\n","    \"lite\": [\n","        \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/latest/pose_landmarker_lite.task\",\n","        \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/1/pose_landmarker_lite.task\",\n","    ],\n","    \"full\": [\n","        \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_full/float16/latest/pose_landmarker_full.task\",\n","        \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_full/float16/1/pose_landmarker_full.task\",\n","    ],\n","    \"heavy\": [\n","        \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/latest/pose_landmarker_heavy.task\",\n","        \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task\",\n","    ],\n","}\n","\n","def download_pose_model(variant: str = \"full\") -> str:\n","    \"\"\"\n","    Download the selected model variant (.task) to MODELS_DIR.\n","    Returns the local file path.\n","    \"\"\"\n","    variant = variant.lower().strip()\n","    assert variant in MODEL_URLS, f\"Unknown variant '{variant}'. Choose: lite, full, heavy.\"\n","\n","    out_path = MODELS_DIR / f\"pose_landmarker_{variant}.task\"\n","    if out_path.exists() and out_path.stat().st_size > 50_000:\n","        print(f\"‚úî Model already present: {out_path}\")\n","        return str(out_path)\n","\n","    last_err = None\n","    for url in MODEL_URLS[variant]:\n","        try:\n","            print(f\"Downloading {variant} model from:\\n  {url}\")\n","            with urllib.request.urlopen(url, timeout=60) as r, open(out_path, \"wb\") as f:\n","                f.write(r.read())\n","            if out_path.stat().st_size <= 50_000:\n","                raise RuntimeError(\"Downloaded file seems too small; trying fallback...\")\n","            print(f\"‚úî Saved to {out_path} ({out_path.stat().st_size/1e6:.2f} MB)\")\n","            return str(out_path)\n","        except Exception as e:\n","            print(f\"‚Ä¶ failed: {e}\")\n","            last_err = e\n","    raise RuntimeError(f\"Could not download model for variant '{variant}'. Last error: {last_err}\")\n","\n","# ---- Choose your default model here ----\n","MODEL_VARIANT = \"full\"   # options: \"lite\", \"full\", \"heavy\"\n","MODEL_PATH = download_pose_model(MODEL_VARIANT)\n","\n","# ---- Create PoseLandmarker with GPU delegate, fall back to CPU ----\n","BaseOptions = mp_python.BaseOptions\n","PoseLandmarker = mp_vision.PoseLandmarker\n","PoseLandmarkerOptions = mp_vision.PoseLandmarkerOptions\n","RunningMode = mp_vision.RunningMode\n","\n","def create_landmarker(delegate: BaseOptions.Delegate):\n","    options = PoseLandmarkerOptions(\n","        base_options=BaseOptions(\n","            model_asset_path=MODEL_PATH,\n","            delegate=delegate\n","        ),\n","        running_mode=RunningMode.VIDEO,\n","        num_poses=1,\n","        min_pose_detection_confidence=0.5,\n","        min_pose_presence_confidence=0.5,\n","        min_tracking_confidence=0.5,\n","        output_segmentation_masks=False,\n","    )\n","    return PoseLandmarker.create_from_options(options)\n","\n","landmarker = None\n","try:\n","    landmarker = create_landmarker(BaseOptions.Delegate.GPU)\n","    print(\"‚úÖ PoseLandmarker initialized with GPU delegate.\")\n","except Exception as e:\n","    print(f\"‚ö†Ô∏è GPU delegate unavailable ({e}). Falling back to CPU‚Ä¶\")\n","    landmarker = create_landmarker(BaseOptions.Delegate.CPU)\n","    print(\"‚úÖ PoseLandmarker initialized with CPU delegate.\")\n","\n","print(f\"   Model: {MODEL_VARIANT} ‚Üí {MODEL_PATH}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hf6bTmg1cdx5","executionInfo":{"status":"ok","timestamp":1762814532920,"user_tz":300,"elapsed":616,"user":{"displayName":"Souvik Mandal","userId":"02704498209737744010"}},"outputId":"22f15971-b6fc-4e89-9eb2-149a24fba98a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading full model from:\n","  https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_full/float16/latest/pose_landmarker_full.task\n","‚úî Saved to models/pose_landmarker_full.task (9.40 MB)\n","‚ö†Ô∏è GPU delegate unavailable (Service \"kGpuService\", required by node mediapipe_tasks_vision_pose_landmarker_poselandmarkergraph__mediapipe_tasks_vision_pose_detector_posedetectorgraph__mediapipe_tasks_core_inferencesubgraph__inferencecalculator__mediapipe_tasks_vision_pose_landmarker_poselandmarkergraph__mediapipe_tasks_vision_pose_detector_posedetectorgraph__mediapipe_tasks_core_inferencesubgraph__InferenceCalculator, was not provided and cannot be created: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:84) egl_initializedUnable to initialize EGL). Falling back to CPU‚Ä¶\n","‚úÖ PoseLandmarker initialized with CPU delegate.\n","   Model: full ‚Üí models/pose_landmarker_full.task\n"]}]},{"cell_type":"code","source":["# =========================================\n","# 2.1 Parameters ‚Äî YOU control the pipeline\n","# =========================================\n","\n","# --- Model choice ---\n","MODEL_VARIANT = \"lite\"          # options: \"lite\", \"full\", \"heavy\"\n","\n","# --- Inference behavior ---\n","frame_stride = 1                 # 1=every frame; 2=every other; 3=every third; etc.\n","num_poses = 1                    # usually 1 for single-person videos\n","min_pose_detection_confidence = 0.5\n","min_pose_presence_confidence  = 0.5\n","min_tracking_confidence       = 0.5\n","\n","# --- Outputs ---\n","make_annotated_video = True      # set False to skip saving annotated MP4\n","out_dir = \"outputs\"              # where CSVs / MP4 will be written\n","\n","# --- Sync the model file if variant changed ---\n","# Assumes the download_pose_model() function from earlier cells is defined.\n","def ensure_model_variant(variant: str) -> str:\n","    v = variant.strip().lower()\n","    if v not in (\"lite\", \"full\", \"heavy\"):\n","        raise ValueError(\"MODEL_VARIANT must be one of: 'lite', 'full', 'heavy'.\")\n","    return download_pose_model(v)\n","\n","MODEL_PATH = \"models/pose_landmarker_full.task\"\n","print(f\"‚úî Using model: {MODEL_VARIANT} ‚Üí {MODEL_PATH}\")\n","print(f\"   frame_stride={frame_stride}, num_poses={num_poses}, annotated={make_annotated_video}\")\n","print(f\"   confidences: detection={min_pose_detection_confidence}, presence={min_pose_presence_confidence}, tracking={min_tracking_confidence}\")\n","print(f\"   output dir: {out_dir}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V2B0MG_rchSG","executionInfo":{"status":"ok","timestamp":1762806516602,"user_tz":300,"elapsed":8,"user":{"displayName":"Zachary Buller","userId":"10720990699951540462"}},"outputId":"518775ed-b380-47df-f921-fafd18732412"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úî Using model: lite ‚Üí models/pose_landmarker_full.task\n","   frame_stride=1, num_poses=1, annotated=True\n","   confidences: detection=0.5, presence=0.5, tracking=0.5\n","   output dir: outputs\n"]}]},{"cell_type":"code","source":["# =========================================================\n","# 3.1 Ensure CSVs include landmark_name & add peek helpers\n","# =========================================================\n","\n","# If you already defined POSE_LANDMARK_NAMES & landmark_index_to_name earlier, we reuse them.\n","# If not, define them quickly here:\n","try:\n","    landmark_index_to_name\n","except NameError:\n","    POSE_LANDMARK_NAMES = [\n","        \"nose\",\"left_eye_inner\",\"left_eye\",\"left_eye_outer\",\n","        \"right_eye_inner\",\"right_eye\",\"right_eye_outer\",\n","        \"left_ear\",\"right_ear\",\"mouth_left\",\"mouth_right\",\n","        \"left_shoulder\",\"right_shoulder\",\"left_elbow\",\"right_elbow\",\n","        \"left_wrist\",\"right_wrist\",\"left_pinky\",\"right_pinky\",\n","        \"left_index\",\"right_index\",\"left_thumb\",\"right_thumb\",\n","        \"left_hip\",\"right_hip\",\"left_knee\",\"right_knee\",\n","        \"left_ankle\",\"right_ankle\",\"left_heel\",\"right_heel\",\n","        \"left_foot_index\",\"right_foot_index\",\n","    ]\n","    landmark_index_to_name = {i: n for i, n in enumerate(POSE_LANDMARK_NAMES)}\n","\n","# --- Re-define extract_pose_from_video to include `landmark_name` columns ---\n","def extract_pose_from_video(\n","    video_path: str,\n","    model_path: str,\n","    out_dir: str = \"outputs\",\n","    make_annotated_video: bool = False,\n","    frame_stride: int = 1,\n","    num_poses: int = 1,\n","    min_pose_detection_confidence: float = 0.5,\n","    min_pose_presence_confidence: float = 0.5,\n","    min_tracking_confidence: float = 0.5,\n","    output_segmentation_masks: bool = False,\n","):\n","    import os, pathlib\n","    import cv2, numpy as np, pandas as pd\n","    import mediapipe as mp\n","    from mediapipe.tasks import python as mp_python\n","    from mediapipe.tasks.python import vision as mp_vision\n","\n","    os.makedirs(out_dir, exist_ok=True)\n","    stem = pathlib.Path(video_path).stem\n","    csv2d = os.path.join(out_dir, f\"{stem}_pose2d.csv\")\n","    csv3d = os.path.join(out_dir, f\"{stem}_pose3d.csv\")\n","    mp4_out = os.path.join(out_dir, f\"{stem}_annotated.mp4\")\n","\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        raise FileNotFoundError(f\"Cannot open video: {video_path}\")\n","\n","    fps    = cap.get(cv2.CAP_PROP_FPS) or 30.0\n","    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","    writer = None\n","    if make_annotated_video:\n","        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n","        writer  = cv2.VideoWriter(mp4_out, fourcc, fps / max(1, frame_stride), (width, height))\n","\n","    BaseOptions = mp_python.BaseOptions\n","    PoseLandmarker = mp_vision.PoseLandmarker\n","    PoseLandmarkerOptions = mp_vision.PoseLandmarkerOptions\n","    RunningMode = mp_vision.RunningMode\n","\n","    options = PoseLandmarkerOptions(\n","        base_options=BaseOptions(model_asset_path=model_path),\n","        running_mode=RunningMode.VIDEO,\n","        num_poses=num_poses,\n","        min_pose_detection_confidence=min_pose_detection_confidence,\n","        min_pose_presence_confidence=min_pose_presence_confidence,\n","        min_tracking_confidence=min_tracking_confidence,\n","        output_segmentation_masks=output_segmentation_masks,\n","    )\n","\n","    # Minimal overlay helper (same as earlier cell)\n","    def _mp_image_from_bgr(bgr):\n","        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n","        return mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)\n","\n","    def _draw_skeleton(bgr, norm_landmarks, visibility_thresh=0.5):\n","        h, w = bgr.shape[:2]\n","        pts = {}\n","        for i, lm in enumerate(norm_landmarks):\n","            vis = getattr(lm, \"visibility\", 1.0) or 0.0\n","            if vis >= visibility_thresh:\n","                x, y = int(lm.x * w), int(lm.y * h)\n","                pts[i] = (x, y)\n","                cv2.circle(bgr, (x, y), 2, (255, 255, 255), -1)\n","        for a, b in [\n","            (11,13),(13,15),(12,14),(14,16),(11,12),(23,24),(11,23),(12,24),\n","            (23,25),(25,27),(24,26),(26,28),(27,29),(29,31),(28,30),(30,32)\n","        ]:\n","            if a in pts and b in pts:\n","                cv2.line(bgr, pts[a], pts[b], (255, 255, 255), 2)\n","\n","    rows2d, rows3d = [], []\n","\n","    with PoseLandmarker.create_from_options(options) as landmarker:\n","        frame_idx = 0\n","        while True:\n","            ok, bgr = cap.read()\n","            if not ok:\n","                break\n","            if frame_stride > 1 and (frame_idx % frame_stride != 0):\n","                frame_idx += 1\n","                continue\n","\n","            ts_ms = int((frame_idx / fps) * 1000.0)\n","            mp_image = _mp_image_from_bgr(bgr)\n","            result = landmarker.detect_for_video(mp_image, ts_ms)\n","\n","            for pose_id, nlands in enumerate(result.pose_landmarks):\n","                # 2D\n","                for li, lm in enumerate(nlands):\n","                    rows2d.append({\n","                        \"video\": os.path.basename(video_path),\n","                        \"frame\": frame_idx,\n","                        \"time_ms\": ts_ms,\n","                        \"landmark_index\": li,\n","                        \"landmark_name\": landmark_index_to_name.get(li, str(li)),\n","                        \"x\": lm.x,\n","                        \"y\": lm.y,\n","                        \"z\": lm.z,\n","                        \"visibility\": getattr(lm, \"visibility\", np.nan),\n","                    })\n","                # 3D\n","                if len(result.pose_world_landmarks) > pose_id:\n","                    wlands = result.pose_world_landmarks[pose_id]\n","                    for li, lm in enumerate(wlands):\n","                        rows3d.append({\n","                            \"video\": os.path.basename(video_path),\n","                            \"frame\": frame_idx,\n","                            \"time_ms\": ts_ms,\n","                            \"landmark_index\": li,\n","                            \"landmark_name\": landmark_index_to_name.get(li, str(li)),\n","                            \"x_m\": lm.x,\n","                            \"y_m\": lm.y,\n","                            \"z_m\": lm.z,\n","                            \"visibility\": getattr(lm, \"visibility\", np.nan),\n","                        })\n","\n","                if writer is not None and len(nlands) > 0:\n","                    bgr_draw = bgr.copy()\n","                    _draw_skeleton(bgr_draw, nlands, visibility_thresh=0.5)\n","                    writer.write(bgr_draw)\n","\n","            frame_idx += 1\n","\n","    cap.release()\n","    if writer is not None:\n","        writer.release()\n","\n","    pd.DataFrame(rows2d).to_csv(csv2d, index=False)\n","    if rows3d:\n","        pd.DataFrame(rows3d).to_csv(csv3d, index=False)\n","    else:\n","        csv3d = None\n","\n","    return {\"csv2d\": csv2d, \"csv3d\": csv3d, \"annotated_mp4\": (mp4_out if make_annotated_video else None)}\n","\n","# --- Quick peek helper ---\n","def peek_csv(path, n=5):\n","    import pandas as pd\n","    df = pd.read_csv(path)\n","    print(f\"{path} ‚Üí shape={df.shape}\")\n","    display(df.head(n))\n","    return df\n","\n","print(\"‚úÖ Extraction function updated to include landmark_name in both CSVs.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xPwAcZ7Zch-n","executionInfo":{"status":"ok","timestamp":1762806535441,"user_tz":300,"elapsed":6,"user":{"displayName":"Zachary Buller","userId":"10720990699951540462"}},"outputId":"9e8ee4a9-ca07-4bdc-93df-9e14354e38d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Extraction function updated to include landmark_name in both CSVs.\n"]}]},{"cell_type":"code","source":["# --- Quick usage example (edit the path) ---\n","# Example call using the ‚Äú2.1 Parameters‚Äù variables the student set:\n","outputs = extract_pose_from_video(\n","    video_path=\"Aryeh_1.MP4\",\n","    model_path=MODEL_PATH,                # from the model download step\n","    out_dir=out_dir,\n","    make_annotated_video=False,\n","    frame_stride=frame_stride,\n","    num_poses=num_poses,\n","    min_pose_detection_confidence=min_pose_detection_confidence,\n","    min_pose_presence_confidence=min_pose_presence_confidence,\n","    min_tracking_confidence=min_tracking_confidence,\n",")\n","outputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":382},"id":"71RPJvITckFQ","executionInfo":{"status":"error","timestamp":1762807032633,"user_tz":300,"elapsed":337068,"user":{"displayName":"Zachary Buller","userId":"10720990699951540462"}},"outputId":"b7168ed2-2f01-425f-a79e-333a750a1b55"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2447949278.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# --- Quick usage example (edit the path) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Example call using the ‚Äú2.1 Parameters‚Äù variables the student set:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m outputs = extract_pose_from_video(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mvideo_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Aryeh_1.MP4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m                \u001b[0;31m# from the model download step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1300509541.py\u001b[0m in \u001b[0;36mextract_pose_from_video\u001b[0;34m(video_path, model_path, out_dir, make_annotated_video, frame_stride, num_poses, min_pose_detection_confidence, min_pose_presence_confidence, min_tracking_confidence, output_segmentation_masks)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mts_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_idx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mmp_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_mp_image_from_bgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbgr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlandmarker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_for_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmp_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts_ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpose_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlands\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_landmarks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mediapipe/tasks/python/vision/pose_landmarker.py\u001b[0m in \u001b[0;36mdetect_for_video\u001b[0;34m(self, image, timestamp_ms, image_processing_options)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mimage_processing_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroi_allowed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     )\n\u001b[0;32m--> 395\u001b[0;31m     output_packets = self._process_video_data({\n\u001b[0m\u001b[1;32m    396\u001b[0m         _IMAGE_IN_STREAM_NAME: packet_creator.create_image(image).at(\n\u001b[1;32m    397\u001b[0m             \u001b[0mtimestamp_ms\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0m_MICRO_SECONDS_PER_MILLISECOND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mediapipe/tasks/python/vision/core/base_vision_task_api.py\u001b[0m in \u001b[0;36m_process_video_data\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    117\u001b[0m           \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m       )\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_live_stream_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Packet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# ============================================\n","# 4. Batch Processing: process a folder of videos\n","# ============================================\n","import os, glob, pathlib, pandas as pd\n","from typing import List, Dict, Optional\n","from tqdm import tqdm\n","\n","# Common video extensions to search for (case-insensitive)\n","VIDEO_EXTS = (\".mp4\", \".mov\", \".m4v\", \".avi\", \".mkv\")\n","\n","def list_videos(input_dir: str, recursive: bool = False) -> List[str]:\n","    \"\"\"Return a sorted list of video file paths in the directory.\"\"\"\n","    p = pathlib.Path(input_dir)\n","    if not p.exists() or not p.is_dir():\n","        raise NotADirectoryError(f\"Not a directory: {input_dir}\")\n","    if recursive:\n","        files = [str(f) for f in p.rglob(\"*\") if f.suffix.lower() in VIDEO_EXTS]\n","    else:\n","        files = [str(f) for f in p.iterdir() if f.is_file() and f.suffix.lower() in VIDEO_EXTS]\n","    return sorted(files)\n","\n","def batch_process_videos(\n","    input_dir: str,\n","    model_path: str,\n","    out_dir: str = \"outputs\",\n","    recursive: bool = False,\n","    # Extraction params (default to Section 2.1 variables if they exist)\n","    make_annotated_video: Optional[bool] = None,\n","    frame_stride: Optional[int] = None,\n","    num_poses: Optional[int] = None,\n","    min_pose_detection_confidence: Optional[float] = None,\n","    min_pose_presence_confidence: Optional[float] = None,\n","    min_tracking_confidence: Optional[float] = None,\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Process all videos in a directory and return a summary table\n","    with paths to the generated CSVs and (optional) annotated MP4s.\n","    \"\"\"\n","    # Pull defaults from the Section 2.1 variables if not provided\n","    globals_fallbacks = {\n","        \"make_annotated_video\": True,\n","        \"frame_stride\": 1,\n","        \"num_poses\": 1,\n","        \"min_pose_detection_confidence\": 0.5,\n","        \"min_pose_presence_confidence\": 0.5,\n","        \"min_tracking_confidence\": 0.5,\n","    }\n","    g = globals()\n","    if make_annotated_video is None: make_annotated_video = g.get(\"make_annotated_video\", globals_fallbacks[\"make_annotated_video\"])\n","    if frame_stride is None: frame_stride = g.get(\"frame_stride\", globals_fallbacks[\"frame_stride\"])\n","    if num_poses is None: num_poses = g.get(\"num_poses\", globals_fallbacks[\"num_poses\"])\n","    if min_pose_detection_confidence is None: min_pose_detection_confidence = g.get(\"min_pose_detection_confidence\", globals_fallbacks[\"min_pose_detection_confidence\"])\n","    if min_pose_presence_confidence is None:  min_pose_presence_confidence  = g.get(\"min_pose_presence_confidence\",  globals_fallbacks[\"min_pose_presence_confidence\"])\n","    if min_tracking_confidence is None:       min_tracking_confidence       = g.get(\"min_tracking_confidence\",       globals_fallbacks[\"min_tracking_confidence\"])\n","\n","    os.makedirs(out_dir, exist_ok=True)\n","    files = list_videos(input_dir, recursive=recursive)\n","    if not files:\n","        raise FileNotFoundError(f\"No supported video files found in: {input_dir}\")\n","\n","    records: List[Dict[str, Optional[str]]] = []\n","    print(f\"Found {len(files)} video(s) in {input_dir} (recursive={recursive}).\\n\")\n","\n","    for fpath in tqdm(files, desc=\"Batch processing\", unit=\"video\"):\n","        try:\n","            outs = extract_pose_from_video(\n","                video_path=fpath,\n","                model_path=model_path,\n","                out_dir=out_dir,\n","                make_annotated_video=make_annotated_video,\n","                frame_stride=frame_stride,\n","                num_poses=num_poses,\n","                min_pose_detection_confidence=min_pose_detection_confidence,\n","                min_pose_presence_confidence=min_pose_presence_confidence,\n","                min_tracking_confidence=min_tracking_confidence,\n","            )\n","            records.append({\n","                \"video\": fpath,\n","                \"csv2d\": outs.get(\"csv2d\"),\n","                \"csv3d\": outs.get(\"csv3d\"),\n","                \"annotated_mp4\": outs.get(\"annotated_mp4\"),\n","                \"status\": \"ok\",\n","                \"error\": \"\",\n","            })\n","        except Exception as e:\n","            records.append({\n","                \"video\": fpath,\n","                \"csv2d\": None,\n","                \"csv3d\": None,\n","                \"annotated_mp4\": None,\n","                \"status\": \"error\",\n","                \"error\": str(e),\n","            })\n","\n","    df = pd.DataFrame.from_records(records)\n","    # Optional: save a manifest for later reference\n","    manifest_path = os.path.join(out_dir, \"batch_manifest.csv\")\n","    df.to_csv(manifest_path, index=False)\n","    print(f\"\\n‚úî Batch complete. Manifest saved to: {manifest_path}\")\n","    display(df)\n","    return df"],"metadata":{"id":"4seOesG5con6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Example usage (edit the path to your folder) ---\n","df_summary = batch_process_videos(\n","     input_dir=\"\",\n","     model_path=MODEL_PATH,    # from model download step\n","     out_dir=out_dir,          # from Section 2.1\n","     recursive=False,          # set True to search subfolders\n"," )\n","df_summary.head()"],"metadata":{"id":"y32dFF2HcrB4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def _angle_between(a, b, c):\n","    # a,b,c are 2D points (x,y) or 3D (x,y,z) ‚Äî here we'll use 2D image coords\n","    a, b, c = np.array(a), np.array(b), np.array(c)\n","    ba = a - b\n","    bc = c - b\n","    cosang = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-9)\n","    cosang = np.clip(cosang, -1.0, 1.0)\n","    return np.degrees(np.arccos(cosang))\n","\n","def compute_joint_angle_csv(csv2d_path: str, joint=(\"left_shoulder\",\"left_elbow\",\"left_wrist\")) -> pd.DataFrame:\n","    df = pd.read_csv(csv2d_path)\n","    # wide pivot: columns like x_left_shoulder, y_left_shoulder, etc.\n","    wide = df.pivot_table(index=[\"video\",\"frame\",\"time_ms\"], columns=\"landmark_name\", values=[\"x\",\"y\"])\n","    # helper to get a point\n","    def P(name):\n","        return np.c_[wide[\"x\"][name].values, wide[\"y\"][name].values]\n","    A,B,C = P(joint[0]), P(joint[1]), P(joint[2])\n","    angles = np.array([_angle_between(a,b,c) for a,b,c in zip(A,B,C)])\n","    out = pd.DataFrame({\n","        \"video\": wide.index.get_level_values(\"video\"),\n","        \"frame\": wide.index.get_level_values(\"frame\"),\n","        \"time_ms\": wide.index.get_level_values(\"time_ms\"),\n","        f\"angle_{'_'.join(joint)}\": angles\n","    })\n","    return out\n","\n","# Example (after extraction):\n","# angle_df = compute_joint_angle_csv(\"outputs/yourvideo_pose2d.csv\", (\"left_shoulder\",\"left_elbow\",\"left_wrist\"))\n","# angle_df.head()"],"metadata":{"id":"yjPNtK5Vctha"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#--- SINGLE VIDEO ---\n","outputs = extract_pose_from_video(\n","    video_path=\"Aryeh_1.MP4\",\n","    model_path=MODEL_PATH,\n","    make_annotated_video=True,\n","    frame_stride=2,   # increase to 2/3 for faster processing\n"," )\n","print(outputs)\n","\n","# --- DIRECTORY ---\n","# df = batch_process_videos(\n","#     input_dir=\"PATH/TO/FOLDER\",\n","#     model_path=MODEL_PATH,\n","#     pattern=\"*.mp4\",\n","#     out_dir=\"outputs\",\n","#     make_annotated_video=False,\n","#     frame_stride=2,\n","# )\n","# df"],"metadata":{"id":"UXMMP9s2cwAx"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"/v2/external/notebooks/intro.ipynb","timestamp":1762805546076}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}