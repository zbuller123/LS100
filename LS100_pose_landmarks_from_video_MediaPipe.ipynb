{"cells":[{"cell_type":"markdown","id":"bbabded9-cf8c-4833-a4be-aab5cb6f98af","metadata":{"id":"bbabded9-cf8c-4833-a4be-aab5cb6f98af"},"source":["# Pose Landmarks with MediaPipe ‚Äî From Local Videos & Folders Using Python\n","\n","This notebook is both a **guided lesson** and a **working pipeline** for detecting human pose landmarks from **local video files** or **entire folders** of videos using **MediaPipe Tasks**.\n","\n","## Goal\n","\n","1. Set up a clean Python 3.12 environment and verify required packages.\n","2. Understand each step and terminologies.\n","3. Download and select a Pose Landmarker model (**lite / full / heavy**) and understand accuracy‚Äìspeed trade-offs.\n","4. Read videos with OpenCV and run inference in **`RunningMode.VIDEO`** with correct **timestamps**.\n","5. Export results as tidy CSVs for analysis: **2D image-normalized** and **3D world** landmarks.\n","6. Create an **annotated MP4** showing the skeleton overlay.\n","7. Build intuition for **visibility**, **image vs. world coordinates**, and simple feature engineering (e.g., joint angles).\n","\n","> **Built for learning:** Along the way you‚Äôll see short callouts explaining *why* each step exists (e.g., timestamps in VIDEO mode), how coordinate spaces differ, and how to tune speed vs. accuracy.\n","\n","## After completing this guide, you will be able to\n","\n","* Load one video‚Äîor loop through an entire folder‚Äîand extract the coordinates of the landmark bodypoints frame-by-frame.\n","* Save two analysis-ready CSVs per video: one for **2D normalized** landmarks and one for **3D world** coordinates.\n","* Produce an **annotated MP4** with landmarks and connections overlaid.\n","* Explain and adjust **`RunningMode.VIDEO`**, **per-frame timestamps**, **visibility filtering**, **image vs. world coordinates**, and model variants (**lite/full/heavy**).\n","\n","> **Prerequisites**\n",">\n","> * Python **3.12** virtual environment selected as the active Jupyter kernel. In case yo8u need help, please refer to the \"LS100_Guide 3_Introduction to Pose Estimation Using MediaPipe.pdf\" guide.\n","> * Installed packages: `mediapipe opencv-python pandas numpy tqdm matplotlib seaborn`\n","> * One or more local video files (e.g., `.mp4`) to test.\n","\n","> **Ethics & consent**\n",">\n","> * If processing videos of people, obtain consent and store data securely. Avoid uploading sensitive content to third-party services.\n","\n","### References for learners\n","\n","* MediaPipe Pose Landmarker (Python guide): [https://ai.google.dev/edge/mediapipe/solutions/vision/pose_landmarker/python](https://ai.google.dev/edge/mediapipe/solutions/vision/pose_landmarker/python)\n","* Pose Landmarker API: [https://ai.google.dev/edge/api/mediapipe/python/mp/tasks/vision/PoseLandmarker](https://ai.google.dev/edge/api/mediapipe/python/mp/tasks/vision/PoseLandmarker)\n","* Model card (BlazePose GHUM 3D; lite/full/heavy):\n","  [https://storage.googleapis.com/mediapipe-assets/Model%20Card%20BlazePose%20GHUM%203D.pdf](https://storage.googleapis.com/mediapipe-assets/Model%20Card%20BlazePose%20GHUM%203D.pdf)\n","\n","---\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OW5O3dC-gfJ9","executionInfo":{"status":"ok","timestamp":1763612702476,"user_tz":300,"elapsed":15673,"user":{"displayName":"Zachary Buller","userId":"10720990699951540462"}},"outputId":"0b1a8384-8ed4-4d80-c2cc-18e0b13069c4"},"id":"OW5O3dC-gfJ9","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","id":"caf62f41-3483-4103-9b1e-51d470dc174d","metadata":{"id":"caf62f41-3483-4103-9b1e-51d470dc174d"},"source":["# 0. Environment Setup and Verification (LS100 Standard)\n","\n","Before running any code, make sure you‚Äôre using the **LS100_PoseEstimation_MP** kernel that was created in your Python 3.12 virtual environment.\n","This section verifies your environment and installs all required packages.\n","\n","---\n","\n","### **What you should already have**\n","\n","‚úÖ Python 3.12 installed\n","\n","‚úÖ Virtual environment activated (`(MediaPipeEnv)` should appear in your terminal)\n","\n","‚úÖ Kernel registered as **LS100_PoseEstimation_MP**\n","\n","If you haven‚Äôt completed those steps, revisit the **LS100_Guide 3_Introduction to Pose Estimation Using MediaPipe.pdf** document.\n","\n","---\n","\n","### **Required packages**\n","\n","This notebook uses the following libraries:\n","\n","* `mediapipe` ‚Äì pose landmark model and API\n","* `opencv-python` ‚Äì video I/O (input/output) and frame conversion\n","* `pandas` & `numpy` ‚Äì data handling and analysis\n","* `tqdm` ‚Äì progress bars for video processing\n","* `matplotlib` & `seaborn` ‚Äì visualization and data inspection\n","\n","Run the next cell to ensure these are installed and to confirm the environment details.\n","\n","---\n","\n","### **Learning focus**\n","\n","* Why virtual environments prevent version conflicts\n","* Why we require **Python 3.12** (MediaPipe Tasks currently supports Python 3.9‚Äì3.12 only)\n","* How each library fits into the MediaPipe Pose pipeline\n","---\n"]},{"cell_type":"markdown","id":"222fe87d","metadata":{"id":"222fe87d"},"source":["\n","## 0. Environment setup\n","\n","> If running locally (VS Code/Jupyter), run the following cell once; it might take about a minute to run.\n"]},{"cell_type":"code","execution_count":null,"id":"64456878","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"64456878","executionInfo":{"status":"ok","timestamp":1763612763488,"user_tz":300,"elapsed":50794,"user":{"displayName":"Zachary Buller","userId":"10720990699951540462"}},"outputId":"259e981c-9b43-4805-e963-6e3b133e9464"},"outputs":[{"output_type":"stream","name":"stdout","text":["üß† Python version: 3.12.12\n","‚¨áÔ∏è Installing mediapipe ...\n","‚¨áÔ∏è Installing opencv-python ...\n","‚úÖ pandas already installed\n","‚úÖ numpy already installed\n","‚úÖ tqdm already installed\n","‚úÖ matplotlib already installed\n","‚úÖ seaborn already installed\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.7.2 is installed, but it is not compatible with the installed jaxlib version 0.7.1, so it will not be used.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","üì¶ Package versions:\n","mediapipe      : 0.10.21\n","opencv-python  : 4.11.0\n","pandas         : 2.2.2\n","numpy          : 2.0.2\n","matplotlib     : 3.10.0\n","seaborn        : 0.13.2\n","\n","‚úÖ Environment is ready to proceed!\n"]}],"source":["# ============================================\n","# 0. Environment Setup and Package Verification\n","# ============================================\n","\n","import sys\n","import importlib\n","import subprocess\n","\n","# ---- 1. Check Python version ----\n","py_version = sys.version_info\n","print(f\"üß† Python version: {py_version.major}.{py_version.minor}.{py_version.micro}\")\n","if py_version < (3, 9) or py_version >= (3, 13):\n","    print(\"‚ö†Ô∏è MediaPipe Tasks officially supports Python 3.9‚Äì3.12.\")\n","    print(\"‚ö†Ô∏è Please switch to Python 3.12 for this notebook (as used in LS100).\")\n","\n","# ---- 2. Define required packages ----\n","required_packages = [\n","    \"mediapipe\",\n","    \"opencv-python\",\n","    \"pandas\",\n","    \"numpy\",\n","    \"tqdm\",\n","    \"matplotlib\",\n","    \"seaborn\",\n","]\n","\n","# ---- 3. Function to check and install ----\n","def install_if_missing(pkg):\n","    \"\"\"\n","    Try importing the package; if not found, install it quietly.\n","    \"\"\"\n","    try:\n","        importlib.import_module(pkg.split(\"==\")[0])\n","        print(f\"‚úÖ {pkg} already installed\")\n","    except ImportError:\n","        print(f\"‚¨áÔ∏è Installing {pkg} ...\")\n","        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n","\n","# ---- 4. Verify each dependency ----\n","for package in required_packages:\n","    install_if_missing(package)\n","\n","# ---- 5. Print package versions for reproducibility ----\n","import mediapipe as mp\n","import cv2, pandas as pd, numpy as np, tqdm, matplotlib, seaborn\n","\n","print(\"\\nüì¶ Package versions:\")\n","print(f\"mediapipe      : {mp.__version__}\")\n","print(f\"opencv-python  : {cv2.__version__}\")\n","print(f\"pandas         : {pd.__version__}\")\n","print(f\"numpy          : {np.__version__}\")\n","print(f\"matplotlib     : {matplotlib.__version__}\")\n","print(f\"seaborn        : {seaborn.__version__}\")\n","\n","print(\"\\n‚úÖ Environment is ready to proceed!\")\n"]},{"cell_type":"markdown","id":"f4ccfbb6","metadata":{"id":"f4ccfbb6"},"source":["\n","## 1. Imports & version checks\n"]},{"cell_type":"markdown","id":"0a7ffc92-aa13-4a39-9a17-6315fc1d1e79","metadata":{"id":"0a7ffc92-aa13-4a39-9a17-6315fc1d1e79"},"source":["---\n","# 1. Imports and Version Verification\n","\n","Now that your environment is ready, let‚Äôs import the main libraries used throughout this notebook.\n","\n","This step helps confirm that:\n","\n","* The correct packages are installed inside your LS100 virtual environment\n","* MediaPipe loads successfully (and we can access its **Tasks API**)\n","* OpenCV, NumPy, and Pandas are working properly\n","\n","If an import fails, it usually means you‚Äôre running the notebook in a different kernel (not the one you registered).\n","You can fix that by selecting **Kernel ‚Üí Change Kernel ‚Üí LS100_PoseEstimation_MP** (or the name you chose).\n","\n","---"]},{"cell_type":"code","execution_count":null,"id":"c1f6603e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c1f6603e","executionInfo":{"status":"ok","timestamp":1763612774396,"user_tz":300,"elapsed":44,"user":{"displayName":"Zachary Buller","userId":"10720990699951540462"}},"outputId":"3331a4fe-473b-4376-da76-af304b4b3304"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ MediaPipe Tasks API imported successfully!\n","\n","mediapipe version : 0.10.21\n","opencv version    : 4.11.0\n","pandas version    : 2.2.2\n","numpy version     : 2.0.2\n","‚öôÔ∏è Running on CPU\n","\n"," MediaPipe Tasks API is available:\n","- BaseOptions           : True\n","- PoseLandmarker        : True\n","- PoseLandmarkerOptions : True\n","- RunningMode           : True\n"]}],"source":["# ======================================\n","# 1. Import Libraries and Verify Versions (fixed for MediaPipe >=0.10)\n","# ======================================\n","\n","import os, cv2, numpy as np, pandas as pd, matplotlib, seaborn as sns\n","from tqdm import tqdm\n","\n","import mediapipe as mp\n","from mediapipe.tasks import python as mp_python\n","from mediapipe.tasks.python import vision as mp_vision\n","\n","print(\"‚úÖ MediaPipe Tasks API imported successfully!\\n\")\n","print(f\"mediapipe version : {mp.__version__}\")\n","print(f\"opencv version    : {cv2.__version__}\")\n","print(f\"pandas version    : {pd.__version__}\")\n","print(f\"numpy version     : {np.__version__}\")\n","\n","# Optional: check GPU availability\n","backend = \"GPU\" if cv2.cuda.getCudaEnabledDeviceCount() > 0 else \"CPU\"\n","print(f\"‚öôÔ∏è Running on {backend}\")\n","\n","# ---- Smoke test: confirm Tasks API symbols exist ----\n","BaseOptions = mp_python.BaseOptions\n","PoseLandmarker = mp_vision.PoseLandmarker\n","PoseLandmarkerOptions = mp_vision.PoseLandmarkerOptions\n","RunningMode = mp_vision.RunningMode\n","\n","print(\"\\n MediaPipe Tasks API is available:\")\n","print(f\"- BaseOptions           : {BaseOptions is not None}\")\n","print(f\"- PoseLandmarker        : {PoseLandmarker is not None}\")\n","print(f\"- PoseLandmarkerOptions : {PoseLandmarkerOptions is not None}\")\n","print(f\"- RunningMode           : {RunningMode is not None}\")"]},{"cell_type":"markdown","id":"c74d16a0-d5a3-4a65-ae1e-09995f8c0820","metadata":{"id":"c74d16a0-d5a3-4a65-ae1e-09995f8c0820"},"source":["---\n","\n","### üí° **Notes**\n","\n","* **Why this matters:** ensures that the environment is truly isolated and reproducible.\n","* **Discussion prompt:** Can you tell *why* we check MediaPipe imports *before* running the pipeline? (to confirm the **Tasks** API is available and working).\n","* **TASK:** Print `mp.__file__` to confirm MediaPipe‚Äôs path. This helps you understand where packages live inside the venv.\n","\n","---"]},{"cell_type":"markdown","id":"e1ca8f9e","metadata":{"id":"e1ca8f9e"},"source":["\n","## 2. How Pose Landmarker works (what to teach)\n","\n","- **Running modes:** `IMAGE`, `VIDEO`, `LIVE_STREAM`. For offline videos we use **`VIDEO`** and must pass a **timestamp (ms)** for each frame; the task uses **tracking** to avoid re-running the full model on every frame (reduces latency at the same accuracy settings).  \n","- **Outputs:**  \n","  - **2D normalized landmarks** in image coordinates (*x,y in [0,1] relative to width/height; z is a depth-like value; visibility in [0,1]*).  \n","  - **3D world landmarks** (meters, origin near hip center; handy for biomechanical features).  \n","- **Variants:** **lite / full / heavy**. Heavier models = more accurate, slower (see model card).  \n","- **Accuracy vs speed knobs:** `num_poses` (usually 1 for single-person), `min_pose_detection_confidence`, `min_pose_presence_confidence`, `min_tracking_confidence`, and **frame stride** (e.g., analyze every 2nd/3rd frame).\n","\n","> We‚Äôll expose all of these transparently in helper functions below.\n"]},{"cell_type":"markdown","id":"69078313","metadata":{"id":"69078313"},"source":["\n","## 3. Download a Pose Landmarker model (`.task` bundle)\n","\n","Choose one of: `\"lite\"`, `\"full\"` (default), `\"heavy\"`.  \n","URLs follow Google‚Äôs published pattern; we try `latest/‚Ä¶` first and then fall back to version `1/‚Ä¶`.\n","\n","> You only need to download once; it will be cached under `models/`.\n"]},{"cell_type":"code","execution_count":null,"id":"70bce5a0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70bce5a0","executionInfo":{"status":"ok","timestamp":1763612790903,"user_tz":300,"elapsed":627,"user":{"displayName":"Zachary Buller","userId":"10720990699951540462"}},"outputId":"d17a75ef-dca7-422f-b55c-97b194a2f7ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading full model from:\n","  https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_full/float16/latest/pose_landmarker_full.task\n","‚úî Saved to models/pose_landmarker_full.task (9.40 MB)\n","‚úÖ PoseLandmarker initialized successfully (VIDEO mode).\n","   Model: full ‚Üí models/pose_landmarker_full.task\n"]}],"source":["# ================================\n","# 2. Model Selection & Download\n","# ================================\n","import os\n","import pathlib\n","import urllib.request\n","import urllib.error\n","\n","import mediapipe as mp\n","from mediapipe.tasks import python as mp_python\n","from mediapipe.tasks.python import vision as mp_vision\n","\n","# ---- Where to save models ----\n","MODELS_DIR = pathlib.Path(\"models\")\n","MODELS_DIR.mkdir(parents=True, exist_ok=True)\n","\n","# ---- Official model URLs (latest, with fallback to v1) ----\n","MODEL_URLS = {\n","    \"lite\": [\n","        \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/latest/pose_landmarker_lite.task\",\n","        \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/1/pose_landmarker_lite.task\",\n","    ],\n","    \"full\": [\n","        \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_full/float16/latest/pose_landmarker_full.task\",\n","        \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_full/float16/1/pose_landmarker_full.task\",\n","    ],\n","    \"heavy\": [\n","        \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/latest/pose_landmarker_heavy.task\",\n","        \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task\",\n","    ],\n","}\n","\n","def download_pose_model(variant: str = \"full\") -> str:\n","    \"\"\"\n","    Download the selected model variant (.task) to MODELS_DIR.\n","    Returns the local file path.\n","    \"\"\"\n","    variant = variant.lower().strip()\n","    assert variant in MODEL_URLS, f\"Unknown variant '{variant}'. Choose: lite, full, heavy.\"\n","\n","    out_path = MODELS_DIR / f\"pose_landmarker_{variant}.task\"\n","    if out_path.exists() and out_path.stat().st_size > 50_000:\n","        print(f\"‚úî Model already present: {out_path}\")\n","        return str(out_path)\n","\n","    last_err = None\n","    for url in MODEL_URLS[variant]:\n","        try:\n","            print(f\"Downloading {variant} model from:\\n  {url}\")\n","            with urllib.request.urlopen(url, timeout=60) as r, open(out_path, \"wb\") as f:\n","                f.write(r.read())\n","            if out_path.stat().st_size <= 50_000:\n","                raise RuntimeError(\"Downloaded file seems too small; trying fallback...\")\n","            print(f\"‚úî Saved to {out_path} ({out_path.stat().st_size/1e6:.2f} MB)\")\n","            return str(out_path)\n","        except Exception as e:\n","            print(f\"‚Ä¶ failed: {e}\")\n","            last_err = e\n","    raise RuntimeError(f\"Could not download model for variant '{variant}'. Last error: {last_err}\")\n","\n","# ---- Choose your default model here ----\n","MODEL_VARIANT = \"full\"   # options: \"lite\", \"full\", \"heavy\"\n","MODEL_PATH = download_pose_model(MODEL_VARIANT)\n","\n","# ---- Verify we can initialize the Pose Landmarker (VIDEO mode) ----\n","BaseOptions = mp_python.BaseOptions\n","PoseLandmarker = mp_vision.PoseLandmarker\n","PoseLandmarkerOptions = mp_vision.PoseLandmarkerOptions\n","RunningMode = mp_vision.RunningMode\n","\n","options = PoseLandmarkerOptions(\n","    base_options=BaseOptions(model_asset_path=MODEL_PATH),\n","    running_mode=RunningMode.VIDEO,\n","    num_poses=1,\n","    min_pose_detection_confidence=0.5,\n","    min_pose_presence_confidence=0.5,\n","    min_tracking_confidence=0.5,\n","    output_segmentation_masks=False,\n",")\n","\n","try:\n","    with PoseLandmarker.create_from_options(options) as landmarker:\n","        print(\"‚úÖ PoseLandmarker initialized successfully (VIDEO mode).\")\n","        print(f\"   Model: {MODEL_VARIANT} ‚Üí {MODEL_PATH}\")\n","except Exception as e:\n","    print(\"‚ùå Failed to initialize PoseLandmarker. Check the model file and MediaPipe version.\")\n","    raise\n"]},{"cell_type":"markdown","id":"bbb2c7cc-0137-4ded-83d2-5879407a66d8","metadata":{"id":"bbb2c7cc-0137-4ded-83d2-5879407a66d8"},"source":["## 4. VIDEO mode: timestamps & inference loop\n","\n","For offline videos, we must use RunningMode.VIDEO and pass a monotonic timestamp (ms) for each frame:\n","\n","* We read frames with OpenCV, compute timestamp_ms = int((frame_idx / fps) * 1000), and call\n","landmarker.detect_for_video(mp_image, timestamp_ms).\n","\n","* The Task returns normalized 2D landmarks (x, y ‚àà [0,1], z depth-like, plus visibility) and world 3D landmarks (x_m, y_m, z_m in meters).\n","\n","* We‚Äôll save tidy CSV files for 2D and 3D landmarks.\n","\n","* We‚Äôll also write an annotated MP4 by drawing a simple skeleton over each frame.\n","\n","#### Parameters you can tune\n","\n","* `MODEL_VARIANT` (lite/full/heavy), `num_poses` (usually 1), `frame_stride` (skip frames for speed),\n","\n","* `min_pose_detection_confidence`, `min_pose_presence_confidence`, `min_tracking_confidence`."]},{"cell_type":"markdown","id":"3c10b5c6-459c-4e66-b185-beea783fe65b","metadata":{"id":"3c10b5c6-459c-4e66-b185-beea783fe65b"},"source":["# 5 Choose Your Parameters\n","\n","Before running extraction, set the **tunable parameters** in the next cell.\n","These variables control the accuracy, speed, and outputs of your pipeline.\n","\n","---\n","\n","### **Key Parameters**\n","\n","* **`MODEL_VARIANT`** ‚Äî choose one of:\n","\n","  * `lite` ‚Üí fastest but lowest accuracy\n","  * `full` ‚Üí balanced and recommended default\n","  * `heavy` ‚Üí most accurate but slower\n","\n","* **`frame_stride`** ‚Äî process every *k*-th frame.\n","\n","  * `1` = analyze every frame (slow but detailed)\n","  * `2` = every other frame (twice as fast)\n","  * `3+` = even faster but less temporal precision\n","\n","* **`num_poses`** ‚Äî number of people to detect per frame.\n","\n","  * Use `1` for single-person videos (default for LS100).\n","\n","* **Confidence thresholds**\n","\n","  * `min_pose_detection_confidence` ‚Üí how certain a pose must be detected\n","  * `min_pose_presence_confidence` ‚Üí confidence threshold for the person‚Äôs presence\n","  * `min_tracking_confidence` ‚Üí how reliably MediaPipe should track the same pose across frames\n","\n","* **`make_annotated_video`** ‚Äî if `True`, saves a new video (`.mp4`) with skeleton overlays.\n","\n","* **`out_dir`** ‚Äî the directory where all **CSVs** and (optional) **annotated MP4s** will be saved.\n","\n","---\n","\n","> üß† **Tip:** If you change `MODEL_VARIANT`, the model will automatically download the correct `.task` file again.\n","> Choose the settings thoughtfully based on your hardware and analysis needs‚Äîstudents with slower laptops should prefer `lite` and/or increase `frame_stride`.\n","\n","---"]},{"cell_type":"code","execution_count":null,"id":"7ab7cc04-d179-4ef8-9f71-b52c61dff4d9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ab7cc04-d179-4ef8-9f71-b52c61dff4d9","executionInfo":{"status":"ok","timestamp":1763612805008,"user_tz":300,"elapsed":4,"user":{"displayName":"Zachary Buller","userId":"10720990699951540462"}},"outputId":"f56391c6-f4d4-4035-a959-1252cc7c4073"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úî Using model: lite ‚Üí models/pose_landmarker_full.task\n","   frame_stride=2, num_poses=1, annotated=True\n","   confidences: detection=0.5, presence=0.5, tracking=0.5\n","   output dir: outputs\n"]}],"source":["# =========================================\n","# 2.1 Parameters ‚Äî YOU control the pipeline\n","# =========================================\n","\n","# --- Model choice ---\n","MODEL_VARIANT = \"lite\"          # options: \"lite\", \"full\", \"heavy\"\n","\n","# --- Inference behavior ---\n","frame_stride = 2                 # 1=every frame; 2=every other; 3=every third; etc.\n","num_poses = 1                    # usually 1 for single-person videos\n","min_pose_detection_confidence = 0.5\n","min_pose_presence_confidence  = 0.5\n","min_tracking_confidence       = 0.5\n","\n","# --- Outputs ---\n","make_annotated_video = True      # set False to skip saving annotated MP4\n","out_dir = \"outputs\"              # where CSVs / MP4 will be written\n","\n","# --- Sync the model file if variant changed ---\n","# Assumes the download_pose_model() function from earlier cells is defined.\n","def ensure_model_variant(variant: str) -> str:\n","    v = variant.strip().lower()\n","    if v not in (\"lite\", \"full\", \"heavy\"):\n","        raise ValueError(\"MODEL_VARIANT must be one of: 'lite', 'full', 'heavy'.\")\n","    return download_pose_model(v)\n","\n","MODEL_PATH = \"models/pose_landmarker_full.task\"\n","print(f\"‚úî Using model: {MODEL_VARIANT} ‚Üí {MODEL_PATH}\")\n","print(f\"   frame_stride={frame_stride}, num_poses={num_poses}, annotated={make_annotated_video}\")\n","print(f\"   confidences: detection={min_pose_detection_confidence}, presence={min_pose_presence_confidence}, tracking={min_tracking_confidence}\")\n","print(f\"   output dir: {out_dir}\")\n"]},{"cell_type":"code","execution_count":null,"id":"c3f1b8b8-5c7a-4c9e-83d3-a99de9e7ac87","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c3f1b8b8-5c7a-4c9e-83d3-a99de9e7ac87","executionInfo":{"status":"ok","timestamp":1763612815869,"user_tz":300,"elapsed":12,"user":{"displayName":"Zachary Buller","userId":"10720990699951540462"}},"outputId":"aa8fef30-18d9-4e53-e2cc-995272171a72"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Extraction function updated to include landmark_name in both CSVs.\n"]}],"source":["# =========================================================\n","# 3.1 Ensure CSVs include landmark_name & add peek helpers\n","# =========================================================\n","\n","# If you already defined POSE_LANDMARK_NAMES & landmark_index_to_name earlier, we reuse them.\n","# If not, define them quickly here:\n","try:\n","    landmark_index_to_name\n","except NameError:\n","    POSE_LANDMARK_NAMES = [\n","        \"nose\",\"left_eye_inner\",\"left_eye\",\"left_eye_outer\",\n","        \"right_eye_inner\",\"right_eye\",\"right_eye_outer\",\n","        \"left_ear\",\"right_ear\",\"mouth_left\",\"mouth_right\",\n","        \"left_shoulder\",\"right_shoulder\",\"left_elbow\",\"right_elbow\",\n","        \"left_wrist\",\"right_wrist\",\"left_pinky\",\"right_pinky\",\n","        \"left_index\",\"right_index\",\"left_thumb\",\"right_thumb\",\n","        \"left_hip\",\"right_hip\",\"left_knee\",\"right_knee\",\n","        \"left_ankle\",\"right_ankle\",\"left_heel\",\"right_heel\",\n","        \"left_foot_index\",\"right_foot_index\",\n","    ]\n","    landmark_index_to_name = {i: n for i, n in enumerate(POSE_LANDMARK_NAMES)}\n","\n","# --- Re-define extract_pose_from_video to include `landmark_name` columns ---\n","def extract_pose_from_video(\n","    video_path: str,\n","    model_path: str,\n","    out_dir: str = \"outputs\",\n","    make_annotated_video: bool = False,\n","    frame_stride: int = 1,\n","    num_poses: int = 1,\n","    min_pose_detection_confidence: float = 0.5,\n","    min_pose_presence_confidence: float = 0.5,\n","    min_tracking_confidence: float = 0.5,\n","    output_segmentation_masks: bool = False,\n","):\n","    import os, pathlib\n","    import cv2, numpy as np, pandas as pd\n","    import mediapipe as mp\n","    from mediapipe.tasks import python as mp_python\n","    from mediapipe.tasks.python import vision as mp_vision\n","\n","    os.makedirs(out_dir, exist_ok=True)\n","    stem = pathlib.Path(video_path).stem\n","    csv2d = os.path.join(out_dir, f\"{stem}_pose2d.csv\")\n","    csv3d = os.path.join(out_dir, f\"{stem}_pose3d.csv\")\n","    mp4_out = os.path.join(out_dir, f\"{stem}_annotated.mp4\")\n","\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        raise FileNotFoundError(f\"Cannot open video: {video_path}\")\n","\n","    fps    = cap.get(cv2.CAP_PROP_FPS) or 30.0\n","    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","    writer = None\n","    if make_annotated_video:\n","        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n","        writer  = cv2.VideoWriter(mp4_out, fourcc, fps / max(1, frame_stride), (width, height))\n","\n","    BaseOptions = mp_python.BaseOptions\n","    PoseLandmarker = mp_vision.PoseLandmarker\n","    PoseLandmarkerOptions = mp_vision.PoseLandmarkerOptions\n","    RunningMode = mp_vision.RunningMode\n","\n","    options = PoseLandmarkerOptions(\n","        base_options=BaseOptions(model_asset_path=model_path),\n","        running_mode=RunningMode.VIDEO,\n","        num_poses=num_poses,\n","        min_pose_detection_confidence=min_pose_detection_confidence,\n","        min_pose_presence_confidence=min_pose_presence_confidence,\n","        min_tracking_confidence=min_tracking_confidence,\n","        output_segmentation_masks=output_segmentation_masks,\n","    )\n","\n","    # Minimal overlay helper (same as earlier cell)\n","    def _mp_image_from_bgr(bgr):\n","        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n","        return mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)\n","\n","    def _draw_skeleton(bgr, norm_landmarks, visibility_thresh=0.5):\n","        h, w = bgr.shape[:2]\n","        pts = {}\n","        for i, lm in enumerate(norm_landmarks):\n","            vis = getattr(lm, \"visibility\", 1.0) or 0.0\n","            if vis >= visibility_thresh:\n","                x, y = int(lm.x * w), int(lm.y * h)\n","                pts[i] = (x, y)\n","                cv2.circle(bgr, (x, y), 2, (255, 255, 255), -1)\n","        for a, b in [\n","            (11,13),(13,15),(12,14),(14,16),(11,12),(23,24),(11,23),(12,24),\n","            (23,25),(25,27),(24,26),(26,28),(27,29),(29,31),(28,30),(30,32)\n","        ]:\n","            if a in pts and b in pts:\n","                cv2.line(bgr, pts[a], pts[b], (255, 255, 255), 2)\n","\n","    rows2d, rows3d = [], []\n","\n","    with PoseLandmarker.create_from_options(options) as landmarker:\n","        frame_idx = 0\n","        while True:\n","            ok, bgr = cap.read()\n","            if not ok:\n","                break\n","            if frame_stride > 1 and (frame_idx % frame_stride != 0):\n","                frame_idx += 1\n","                continue\n","\n","            ts_ms = int((frame_idx / fps) * 1000.0)\n","            mp_image = _mp_image_from_bgr(bgr)\n","            result = landmarker.detect_for_video(mp_image, ts_ms)\n","\n","            for pose_id, nlands in enumerate(result.pose_landmarks):\n","                # 2D\n","                for li, lm in enumerate(nlands):\n","                    rows2d.append({\n","                        \"video\": os.path.basename(video_path),\n","                        \"frame\": frame_idx,\n","                        \"time_ms\": ts_ms,\n","                        \"landmark_index\": li,\n","                        \"landmark_name\": landmark_index_to_name.get(li, str(li)),\n","                        \"x\": lm.x,\n","                        \"y\": lm.y,\n","                        \"z\": lm.z,\n","                        \"visibility\": getattr(lm, \"visibility\", np.nan),\n","                    })\n","                # 3D\n","                if len(result.pose_world_landmarks) > pose_id:\n","                    wlands = result.pose_world_landmarks[pose_id]\n","                    for li, lm in enumerate(wlands):\n","                        rows3d.append({\n","                            \"video\": os.path.basename(video_path),\n","                            \"frame\": frame_idx,\n","                            \"time_ms\": ts_ms,\n","                            \"landmark_index\": li,\n","                            \"landmark_name\": landmark_index_to_name.get(li, str(li)),\n","                            \"x_m\": lm.x,\n","                            \"y_m\": lm.y,\n","                            \"z_m\": lm.z,\n","                            \"visibility\": getattr(lm, \"visibility\", np.nan),\n","                        })\n","\n","                if writer is not None and len(nlands) > 0:\n","                    bgr_draw = bgr.copy()\n","                    _draw_skeleton(bgr_draw, nlands, visibility_thresh=0.5)\n","                    writer.write(bgr_draw)\n","\n","            frame_idx += 1\n","\n","    cap.release()\n","    if writer is not None:\n","        writer.release()\n","\n","    pd.DataFrame(rows2d).to_csv(csv2d, index=False)\n","    if rows3d:\n","        pd.DataFrame(rows3d).to_csv(csv3d, index=False)\n","    else:\n","        csv3d = None\n","\n","    return {\"csv2d\": csv2d, \"csv3d\": csv3d, \"annotated_mp4\": (mp4_out if make_annotated_video else None)}\n","\n","# --- Quick peek helper ---\n","def peek_csv(path, n=5):\n","    import pandas as pd\n","    df = pd.read_csv(path)\n","    print(f\"{path} ‚Üí shape={df.shape}\")\n","    display(df.head(n))\n","    return df\n","\n","print(\"‚úÖ Extraction function updated to include landmark_name in both CSVs.\")\n"]},{"cell_type":"code","execution_count":11,"id":"45793be1-7578-42b3-af37-52a96a9a585a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"45793be1-7578-42b3-af37-52a96a9a585a","executionInfo":{"status":"ok","timestamp":1763613468665,"user_tz":300,"elapsed":89860,"user":{"displayName":"Zachary Buller","userId":"10720990699951540462"}},"outputId":"6b0f4abd-ba7c-4c7a-b1e5-e14319acf376"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'csv2d': 'outputs/Aryeh_1_clip1_pose2d.csv',\n"," 'csv3d': 'outputs/Aryeh_1_clip1_pose3d.csv',\n"," 'annotated_mp4': None}"]},"metadata":{},"execution_count":11}],"source":["# --- Quick usage example (edit the path) ---\n","# Example call using the ‚Äú2.1 Parameters‚Äù variables the student set:\n","outputs = extract_pose_from_video(\n","    video_path=\"/content/drive/MyDrive/Harvard/LS100/videos/Frame_Reduced/clips/Aryeh_1_clip1.MP4\", # Corrected to a specific video file\n","    model_path=MODEL_PATH,                # from the model download step\n","    out_dir=out_dir,\n","    make_annotated_video=False,\n","    frame_stride=frame_stride,\n","    num_poses=num_poses,\n","    min_pose_detection_confidence=min_pose_detection_confidence,\n","    min_pose_presence_confidence=min_pose_presence_confidence,\n","    min_tracking_confidence=min_tracking_confidence,\n",")\n","outputs"]},{"cell_type":"markdown","id":"57e10f64","metadata":{"id":"57e10f64"},"source":["\n","> **Notes for students**\n","> - **2D normalized coordinates**: `x,y‚àà[0,1]` relative to image width/height (values can be outside the range if the estimated point is out-of-frame). `z` is depthlike (negative is closer).\n","> - **3D world coordinates**: `x,y,z` are in **meters** in a world coordinate space centered near the hips.  \n","> - **visibility**: confidence for each landmark‚Äôs presence in the frame.\n"]},{"cell_type":"markdown","id":"5dacf32f","metadata":{"id":"5dacf32f"},"source":["\n","## 6. Batch mode ‚Äî process a directory of videos\n","\n","Set `input_dir` and an optional `glob` pattern (e.g., `\"*.mp4\"`). Each video will\n","produce two CSVs (2D + 3D) and, if enabled, an annotated MP4.\n"]},{"cell_type":"code","execution_count":12,"id":"58da1caf-5389-4453-8ac7-3b4921cd47ba","metadata":{"id":"58da1caf-5389-4453-8ac7-3b4921cd47ba","executionInfo":{"status":"ok","timestamp":1763613512435,"user_tz":300,"elapsed":3,"user":{"displayName":"Zachary Buller","userId":"10720990699951540462"}}},"outputs":[],"source":["# ============================================\n","# 4. Batch Processing: process a folder of videos\n","# ============================================\n","import os, glob, pathlib, pandas as pd\n","from typing import List, Dict, Optional\n","from tqdm import tqdm\n","\n","# Common video extensions to search for (case-insensitive)\n","VIDEO_EXTS = (\".mp4\", \".mov\", \".m4v\", \".avi\", \".mkv\")\n","\n","def list_videos(input_dir: str, recursive: bool = False) -> List[str]:\n","    \"\"\"Return a sorted list of video file paths in the directory.\"\"\"\n","    p = pathlib.Path(input_dir)\n","    if not p.exists() or not p.is_dir():\n","        raise NotADirectoryError(f\"Not a directory: {input_dir}\")\n","    if recursive:\n","        files = [str(f) for f in p.rglob(\"*\") if f.suffix.lower() in VIDEO_EXTS]\n","    else:\n","        files = [str(f) for f in p.iterdir() if f.is_file() and f.suffix.lower() in VIDEO_EXTS]\n","    return sorted(files)\n","\n","def batch_process_videos(\n","    input_dir: str,\n","    model_path: str,\n","    out_dir: str = \"outputs\",\n","    recursive: bool = False,\n","    # Extraction params (default to Section 2.1 variables if they exist)\n","    make_annotated_video: Optional[bool] = None,\n","    frame_stride: Optional[int] = None,\n","    num_poses: Optional[int] = None,\n","    min_pose_detection_confidence: Optional[float] = None,\n","    min_pose_presence_confidence: Optional[float] = None,\n","    min_tracking_confidence: Optional[float] = None,\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Process all videos in a directory and return a summary table\n","    with paths to the generated CSVs and (optional) annotated MP4s.\n","    \"\"\"\n","    # Pull defaults from the Section 2.1 variables if not provided\n","    globals_fallbacks = {\n","        \"make_annotated_video\": True,\n","        \"frame_stride\": 1,\n","        \"num_poses\": 1,\n","        \"min_pose_detection_confidence\": 0.5,\n","        \"min_pose_presence_confidence\": 0.5,\n","        \"min_tracking_confidence\": 0.5,\n","    }\n","    g = globals()\n","    if make_annotated_video is None: make_annotated_video = g.get(\"make_annotated_video\", globals_fallbacks[\"make_annotated_video\"])\n","    if frame_stride is None: frame_stride = g.get(\"frame_stride\", globals_fallbacks[\"frame_stride\"])\n","    if num_poses is None: num_poses = g.get(\"num_poses\", globals_fallbacks[\"num_poses\"])\n","    if min_pose_detection_confidence is None: min_pose_detection_confidence = g.get(\"min_pose_detection_confidence\", globals_fallbacks[\"min_pose_detection_confidence\"])\n","    if min_pose_presence_confidence is None:  min_pose_presence_confidence  = g.get(\"min_pose_presence_confidence\",  globals_fallbacks[\"min_pose_presence_confidence\"])\n","    if min_tracking_confidence is None:       min_tracking_confidence       = g.get(\"min_tracking_confidence\",       globals_fallbacks[\"min_tracking_confidence\"])\n","\n","    os.makedirs(out_dir, exist_ok=True)\n","    files = list_videos(input_dir, recursive=recursive)\n","    if not files:\n","        raise FileNotFoundError(f\"No supported video files found in: {input_dir}\")\n","\n","    records: List[Dict[str, Optional[str]]] = []\n","    print(f\"Found {len(files)} video(s) in {input_dir} (recursive={recursive}).\\n\")\n","\n","    for fpath in tqdm(files, desc=\"Batch processing\", unit=\"video\"):\n","        try:\n","            outs = extract_pose_from_video(\n","                video_path=fpath,\n","                model_path=model_path,\n","                out_dir=out_dir,\n","                make_annotated_video=make_annotated_video,\n","                frame_stride=frame_stride,\n","                num_poses=num_poses,\n","                min_pose_detection_confidence=min_pose_detection_confidence,\n","                min_pose_presence_confidence=min_pose_presence_confidence,\n","                min_tracking_confidence=min_tracking_confidence,\n","            )\n","            records.append({\n","                \"video\": fpath,\n","                \"csv2d\": outs.get(\"csv2d\"),\n","                \"csv3d\": outs.get(\"csv3d\"),\n","                \"annotated_mp4\": outs.get(\"annotated_mp4\"),\n","                \"status\": \"ok\",\n","                \"error\": \"\",\n","            })\n","        except Exception as e:\n","            records.append({\n","                \"video\": fpath,\n","                \"csv2d\": None,\n","                \"csv3d\": None,\n","                \"annotated_mp4\": None,\n","                \"status\": \"error\",\n","                \"error\": str(e),\n","            })\n","\n","    df = pd.DataFrame.from_records(records)\n","    # Optional: save a manifest for later reference\n","    manifest_path = os.path.join(out_dir, \"batch_manifest.csv\")\n","    df.to_csv(manifest_path, index=False)\n","    print(f\"\\n‚úî Batch complete. Manifest saved to: {manifest_path}\")\n","    display(df)\n","    return df\n"]},{"cell_type":"code","execution_count":13,"id":"fda3d6fb-7427-46c7-bb8e-2a0bebf45a23","metadata":{"id":"fda3d6fb-7427-46c7-bb8e-2a0bebf45a23","colab":{"base_uri":"https://localhost:8080/","height":818},"executionInfo":{"status":"ok","timestamp":1763615306397,"user_tz":300,"elapsed":1740797,"user":{"displayName":"Zachary Buller","userId":"10720990699951540462"}},"outputId":"8d7cf75b-9850-4b26-a577-f934fce4f255"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 15 video(s) in /content/drive/MyDrive/Harvard/LS100/videos/Frame_Reduced/clips (recursive=False).\n","\n"]},{"output_type":"stream","name":"stderr","text":["Batch processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [29:00<00:00, 116.04s/video]"]},{"output_type":"stream","name":"stdout","text":["\n","‚úî Batch complete. Manifest saved to: outputs/batch_manifest.csv\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"display_data","data":{"text/plain":["                                                video  \\\n","0   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","1   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","2   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","3   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","4   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","5   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","6   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","7   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","8   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","9   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","10  /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","11  /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","12  /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","13  /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","14  /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","\n","                               csv2d                             csv3d  \\\n","0   outputs/Aryeh_1_clip1_pose2d.csv  outputs/Aryeh_1_clip1_pose3d.csv   \n","1   outputs/Aryeh_1_clip2_pose2d.csv  outputs/Aryeh_1_clip2_pose3d.csv   \n","2   outputs/Aryeh_1_clip3_pose2d.csv  outputs/Aryeh_1_clip3_pose3d.csv   \n","3   outputs/Aryeh_2_clip4_pose2d.csv  outputs/Aryeh_2_clip4_pose3d.csv   \n","4   outputs/Aryeh_2_clip5_pose2d.csv  outputs/Aryeh_2_clip5_pose3d.csv   \n","5   outputs/Danny_1_clip1_pose2d.csv  outputs/Danny_1_clip1_pose3d.csv   \n","6   outputs/Danny_1_clip2_pose2d.csv  outputs/Danny_1_clip2_pose3d.csv   \n","7   outputs/Danny_1_clip3_pose2d.csv  outputs/Danny_1_clip3_pose3d.csv   \n","8   outputs/Danny_2_clip4_pose2d.csv  outputs/Danny_2_clip4_pose3d.csv   \n","9   outputs/Danny_2_clip5_pose2d.csv  outputs/Danny_2_clip5_pose3d.csv   \n","10    outputs/Max_1_clip1_pose2d.csv    outputs/Max_1_clip1_pose3d.csv   \n","11    outputs/Max_1_clip2_pose2d.csv    outputs/Max_1_clip2_pose3d.csv   \n","12    outputs/Max_1_clip3_pose2d.csv    outputs/Max_1_clip3_pose3d.csv   \n","13    outputs/Max_2_clip4_pose2d.csv    outputs/Max_2_clip4_pose3d.csv   \n","14    outputs/Max_2_clip5_pose2d.csv    outputs/Max_2_clip5_pose3d.csv   \n","\n","                          annotated_mp4 status error  \n","0   outputs/Aryeh_1_clip1_annotated.mp4     ok        \n","1   outputs/Aryeh_1_clip2_annotated.mp4     ok        \n","2   outputs/Aryeh_1_clip3_annotated.mp4     ok        \n","3   outputs/Aryeh_2_clip4_annotated.mp4     ok        \n","4   outputs/Aryeh_2_clip5_annotated.mp4     ok        \n","5   outputs/Danny_1_clip1_annotated.mp4     ok        \n","6   outputs/Danny_1_clip2_annotated.mp4     ok        \n","7   outputs/Danny_1_clip3_annotated.mp4     ok        \n","8   outputs/Danny_2_clip4_annotated.mp4     ok        \n","9   outputs/Danny_2_clip5_annotated.mp4     ok        \n","10    outputs/Max_1_clip1_annotated.mp4     ok        \n","11    outputs/Max_1_clip2_annotated.mp4     ok        \n","12    outputs/Max_1_clip3_annotated.mp4     ok        \n","13    outputs/Max_2_clip4_annotated.mp4     ok        \n","14    outputs/Max_2_clip5_annotated.mp4     ok        "],"text/html":["\n","  <div id=\"df-1241b2c3-adb4-4ad4-b18d-ba64f4939eeb\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video</th>\n","      <th>csv2d</th>\n","      <th>csv3d</th>\n","      <th>annotated_mp4</th>\n","      <th>status</th>\n","      <th>error</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Aryeh_1_clip1_pose2d.csv</td>\n","      <td>outputs/Aryeh_1_clip1_pose3d.csv</td>\n","      <td>outputs/Aryeh_1_clip1_annotated.mp4</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Aryeh_1_clip2_pose2d.csv</td>\n","      <td>outputs/Aryeh_1_clip2_pose3d.csv</td>\n","      <td>outputs/Aryeh_1_clip2_annotated.mp4</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Aryeh_1_clip3_pose2d.csv</td>\n","      <td>outputs/Aryeh_1_clip3_pose3d.csv</td>\n","      <td>outputs/Aryeh_1_clip3_annotated.mp4</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Aryeh_2_clip4_pose2d.csv</td>\n","      <td>outputs/Aryeh_2_clip4_pose3d.csv</td>\n","      <td>outputs/Aryeh_2_clip4_annotated.mp4</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Aryeh_2_clip5_pose2d.csv</td>\n","      <td>outputs/Aryeh_2_clip5_pose3d.csv</td>\n","      <td>outputs/Aryeh_2_clip5_annotated.mp4</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Danny_1_clip1_pose2d.csv</td>\n","      <td>outputs/Danny_1_clip1_pose3d.csv</td>\n","      <td>outputs/Danny_1_clip1_annotated.mp4</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Danny_1_clip2_pose2d.csv</td>\n","      <td>outputs/Danny_1_clip2_pose3d.csv</td>\n","      <td>outputs/Danny_1_clip2_annotated.mp4</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Danny_1_clip3_pose2d.csv</td>\n","      <td>outputs/Danny_1_clip3_pose3d.csv</td>\n","      <td>outputs/Danny_1_clip3_annotated.mp4</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Danny_2_clip4_pose2d.csv</td>\n","      <td>outputs/Danny_2_clip4_pose3d.csv</td>\n","      <td>outputs/Danny_2_clip4_annotated.mp4</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Danny_2_clip5_pose2d.csv</td>\n","      <td>outputs/Danny_2_clip5_pose3d.csv</td>\n","      <td>outputs/Danny_2_clip5_annotated.mp4</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Max_1_clip1_pose2d.csv</td>\n","      <td>outputs/Max_1_clip1_pose3d.csv</td>\n","      <td>outputs/Max_1_clip1_annotated.mp4</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Max_1_clip2_pose2d.csv</td>\n","      <td>outputs/Max_1_clip2_pose3d.csv</td>\n","      <td>outputs/Max_1_clip2_annotated.mp4</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Max_1_clip3_pose2d.csv</td>\n","      <td>outputs/Max_1_clip3_pose3d.csv</td>\n","      <td>outputs/Max_1_clip3_annotated.mp4</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Max_2_clip4_pose2d.csv</td>\n","      <td>outputs/Max_2_clip4_pose3d.csv</td>\n","      <td>outputs/Max_2_clip4_annotated.mp4</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Max_2_clip5_pose2d.csv</td>\n","      <td>outputs/Max_2_clip5_pose3d.csv</td>\n","      <td>outputs/Max_2_clip5_annotated.mp4</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1241b2c3-adb4-4ad4-b18d-ba64f4939eeb')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-1241b2c3-adb4-4ad4-b18d-ba64f4939eeb button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-1241b2c3-adb4-4ad4-b18d-ba64f4939eeb');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-3fdee6e5-927e-46c1-9994-914dd445e25c\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3fdee6e5-927e-46c1-9994-914dd445e25c')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-3fdee6e5-927e-46c1-9994-914dd445e25c button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"df_summary\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"video\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"/content/drive/MyDrive/Harvard/LS100/videos/Frame_Reduced/clips/Danny_2_clip5.MP4\",\n          \"/content/drive/MyDrive/Harvard/LS100/videos/Frame_Reduced/clips/Max_1_clip2.MP4\",\n          \"/content/drive/MyDrive/Harvard/LS100/videos/Frame_Reduced/clips/Aryeh_1_clip1.MP4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"csv2d\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"outputs/Danny_2_clip5_pose2d.csv\",\n          \"outputs/Max_1_clip2_pose2d.csv\",\n          \"outputs/Aryeh_1_clip1_pose2d.csv\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"csv3d\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"outputs/Danny_2_clip5_pose3d.csv\",\n          \"outputs/Max_1_clip2_pose3d.csv\",\n          \"outputs/Aryeh_1_clip1_pose3d.csv\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"annotated_mp4\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"outputs/Danny_2_clip5_annotated.mp4\",\n          \"outputs/Max_1_clip2_annotated.mp4\",\n          \"outputs/Aryeh_1_clip1_annotated.mp4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"ok\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"error\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["                                               video  \\\n","0  /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","1  /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","2  /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","3  /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","4  /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","\n","                              csv2d                             csv3d  \\\n","0  outputs/Aryeh_1_clip1_pose2d.csv  outputs/Aryeh_1_clip1_pose3d.csv   \n","1  outputs/Aryeh_1_clip2_pose2d.csv  outputs/Aryeh_1_clip2_pose3d.csv   \n","2  outputs/Aryeh_1_clip3_pose2d.csv  outputs/Aryeh_1_clip3_pose3d.csv   \n","3  outputs/Aryeh_2_clip4_pose2d.csv  outputs/Aryeh_2_clip4_pose3d.csv   \n","4  outputs/Aryeh_2_clip5_pose2d.csv  outputs/Aryeh_2_clip5_pose3d.csv   \n","\n","                         annotated_mp4 status error  \n","0  outputs/Aryeh_1_clip1_annotated.mp4     ok        \n","1  outputs/Aryeh_1_clip2_annotated.mp4     ok        \n","2  outputs/Aryeh_1_clip3_annotated.mp4     ok        \n","3  outputs/Aryeh_2_clip4_annotated.mp4     ok        \n","4  outputs/Aryeh_2_clip5_annotated.mp4     ok        "],"text/html":["\n","  <div id=\"df-755e1e30-3289-4871-b01d-88c55a8a38e6\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video</th>\n","      <th>csv2d</th>\n","      <th>csv3d</th>\n","      <th>annotated_mp4</th>\n","      <th>status</th>\n","      <th>error</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Aryeh_1_clip1_pose2d.csv</td>\n","      <td>outputs/Aryeh_1_clip1_pose3d.csv</td>\n","      <td>outputs/Aryeh_1_clip1_annotated.mp4</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Aryeh_1_clip2_pose2d.csv</td>\n","      <td>outputs/Aryeh_1_clip2_pose3d.csv</td>\n","      <td>outputs/Aryeh_1_clip2_annotated.mp4</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Aryeh_1_clip3_pose2d.csv</td>\n","      <td>outputs/Aryeh_1_clip3_pose3d.csv</td>\n","      <td>outputs/Aryeh_1_clip3_annotated.mp4</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Aryeh_2_clip4_pose2d.csv</td>\n","      <td>outputs/Aryeh_2_clip4_pose3d.csv</td>\n","      <td>outputs/Aryeh_2_clip4_annotated.mp4</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Aryeh_2_clip5_pose2d.csv</td>\n","      <td>outputs/Aryeh_2_clip5_pose3d.csv</td>\n","      <td>outputs/Aryeh_2_clip5_annotated.mp4</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-755e1e30-3289-4871-b01d-88c55a8a38e6')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-755e1e30-3289-4871-b01d-88c55a8a38e6 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-755e1e30-3289-4871-b01d-88c55a8a38e6');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-09f154e7-0b53-4656-a36d-d6ce84c87610\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-09f154e7-0b53-4656-a36d-d6ce84c87610')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-09f154e7-0b53-4656-a36d-d6ce84c87610 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_summary","summary":"{\n  \"name\": \"df_summary\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"video\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"/content/drive/MyDrive/Harvard/LS100/videos/Frame_Reduced/clips/Danny_2_clip5.MP4\",\n          \"/content/drive/MyDrive/Harvard/LS100/videos/Frame_Reduced/clips/Max_1_clip2.MP4\",\n          \"/content/drive/MyDrive/Harvard/LS100/videos/Frame_Reduced/clips/Aryeh_1_clip1.MP4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"csv2d\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"outputs/Danny_2_clip5_pose2d.csv\",\n          \"outputs/Max_1_clip2_pose2d.csv\",\n          \"outputs/Aryeh_1_clip1_pose2d.csv\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"csv3d\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"outputs/Danny_2_clip5_pose3d.csv\",\n          \"outputs/Max_1_clip2_pose3d.csv\",\n          \"outputs/Aryeh_1_clip1_pose3d.csv\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"annotated_mp4\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"outputs/Danny_2_clip5_annotated.mp4\",\n          \"outputs/Max_1_clip2_annotated.mp4\",\n          \"outputs/Aryeh_1_clip1_annotated.mp4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"ok\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"error\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":13}],"source":["# --- Example usage (edit the path to your folder) ---\n","df_summary = batch_process_videos(\n","     input_dir=\"/content/drive/MyDrive/Harvard/LS100/videos/Frame_Reduced/clips\",\n","     model_path=MODEL_PATH,    # from model download step\n","     out_dir=out_dir,          # from Section 2.1\n","     recursive=False,          # set True to search subfolders\n"," )\n","df_summary.head()"]},{"cell_type":"markdown","id":"87b2a75b","metadata":{"id":"87b2a75b"},"source":["\n","## 7. Optional: compute simple joint angles\n","\n","Once you have landmarks, you can compute feature engineering targets like **elbow** or **knee angles**. Below is a tiny utility to compute an angle between three named landmarks per frame.\n"]},{"cell_type":"code","execution_count":14,"id":"135b124a","metadata":{"id":"135b124a","executionInfo":{"status":"ok","timestamp":1763615724267,"user_tz":300,"elapsed":41,"user":{"displayName":"Zachary Buller","userId":"10720990699951540462"}}},"outputs":[],"source":["\n","def _angle_between(a, b, c):\n","    # a,b,c are 2D points (x,y) or 3D (x,y,z) ‚Äî here we'll use 2D image coords\n","    a, b, c = np.array(a), np.array(b), np.array(c)\n","    ba = a - b\n","    bc = c - b\n","    cosang = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-9)\n","    cosang = np.clip(cosang, -1.0, 1.0)\n","    return np.degrees(np.arccos(cosang))\n","\n","def compute_joint_angle_csv(csv2d_path: str, joint=(\"left_shoulder\",\"left_elbow\",\"left_wrist\")) -> pd.DataFrame:\n","    df = pd.read_csv(csv2d_path)\n","    # wide pivot: columns like x_left_shoulder, y_left_shoulder, etc.\n","    wide = df.pivot_table(index=[\"video\",\"frame\",\"time_ms\"], columns=\"landmark_name\", values=[\"x\",\"y\"])\n","    # helper to get a point\n","    def P(name):\n","        return np.c_[wide[\"x\"][name].values, wide[\"y\"][name].values]\n","    A,B,C = P(joint[0]), P(joint[1]), P(joint[2])\n","    angles = np.array([_angle_between(a,b,c) for a,b,c in zip(A,B,C)])\n","    out = pd.DataFrame({\n","        \"video\": wide.index.get_level_values(\"video\"),\n","        \"frame\": wide.index.get_level_values(\"frame\"),\n","        \"time_ms\": wide.index.get_level_values(\"time_ms\"),\n","        f\"angle_{'_'.join(joint)}\": angles\n","    })\n","    return out\n","\n","# Example (after extraction):\n","# angle_df = compute_joint_angle_csv(\"outputs/yourvideo_pose2d.csv\", (\"left_shoulder\",\"left_elbow\",\"left_wrist\"))\n","# angle_df.head()\n"]},{"cell_type":"markdown","id":"b71b8f43","metadata":{"id":"b71b8f43"},"source":["\n","## 8. Notes & best practices\n","\n","- **Timestamps matter:** in `VIDEO` mode you *must* pass `timestamp_ms` that increases with frames; we compute it from frame index and FPS.  \n","- **Tracking saves compute:** in `VIDEO`/`LIVE_STREAM` the task performs pose tracking so the full model isn‚Äôt re-run every frame (helps latency).  \n","- **Out-of-frame landmarks:** 2D normalized `x,y` can be outside `[0,1]` if a joint is off‚Äëscreen; use `visibility` to filter.  \n","- **Model choice:** start with **full**, switch to **lite** for underpowered laptops or large batches, use **heavy** when you need the highest accuracy and can afford the speed.  \n","- **Stride:** a cheap speedup is `frame_stride=2` (¬Ω the frames) or higher.  \n","- **Ethics & consent:** if students process videos of people, teach consent, privacy, and secure storage.\n"]},{"cell_type":"markdown","id":"4d86a5ce","metadata":{"id":"4d86a5ce"},"source":["\n","## 9. Quick start (edit the path and run)\n","\n","Uncomment one of the calls below and set your paths.\n"]},{"cell_type":"code","execution_count":18,"id":"c6746b13","metadata":{"id":"c6746b13","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1763617831478,"user_tz":300,"elapsed":1311422,"user":{"displayName":"Zachary Buller","userId":"10720990699951540462"}},"outputId":"143f6fd5-44fe-41a6-fc82-678e133aaff1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 15 video(s) in /content/drive/MyDrive/Harvard/LS100/videos/Frame_Reduced/clips (recursive=False).\n","\n"]},{"output_type":"stream","name":"stderr","text":["Batch processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [21:51<00:00, 87.42s/video]"]},{"output_type":"stream","name":"stdout","text":["\n","‚úî Batch complete. Manifest saved to: outputs/batch_manifest.csv\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"display_data","data":{"text/plain":["                                                video  \\\n","0   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","1   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","2   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","3   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","4   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","5   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","6   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","7   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","8   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","9   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","10  /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","11  /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","12  /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","13  /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","14  /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","\n","                               csv2d                             csv3d  \\\n","0   outputs/Aryeh_1_clip1_pose2d.csv  outputs/Aryeh_1_clip1_pose3d.csv   \n","1   outputs/Aryeh_1_clip2_pose2d.csv  outputs/Aryeh_1_clip2_pose3d.csv   \n","2   outputs/Aryeh_1_clip3_pose2d.csv  outputs/Aryeh_1_clip3_pose3d.csv   \n","3   outputs/Aryeh_2_clip4_pose2d.csv  outputs/Aryeh_2_clip4_pose3d.csv   \n","4   outputs/Aryeh_2_clip5_pose2d.csv  outputs/Aryeh_2_clip5_pose3d.csv   \n","5   outputs/Danny_1_clip1_pose2d.csv  outputs/Danny_1_clip1_pose3d.csv   \n","6   outputs/Danny_1_clip2_pose2d.csv  outputs/Danny_1_clip2_pose3d.csv   \n","7   outputs/Danny_1_clip3_pose2d.csv  outputs/Danny_1_clip3_pose3d.csv   \n","8   outputs/Danny_2_clip4_pose2d.csv  outputs/Danny_2_clip4_pose3d.csv   \n","9   outputs/Danny_2_clip5_pose2d.csv  outputs/Danny_2_clip5_pose3d.csv   \n","10    outputs/Max_1_clip1_pose2d.csv    outputs/Max_1_clip1_pose3d.csv   \n","11    outputs/Max_1_clip2_pose2d.csv    outputs/Max_1_clip2_pose3d.csv   \n","12    outputs/Max_1_clip3_pose2d.csv    outputs/Max_1_clip3_pose3d.csv   \n","13    outputs/Max_2_clip4_pose2d.csv    outputs/Max_2_clip4_pose3d.csv   \n","14    outputs/Max_2_clip5_pose2d.csv    outputs/Max_2_clip5_pose3d.csv   \n","\n","   annotated_mp4 status error  \n","0           None     ok        \n","1           None     ok        \n","2           None     ok        \n","3           None     ok        \n","4           None     ok        \n","5           None     ok        \n","6           None     ok        \n","7           None     ok        \n","8           None     ok        \n","9           None     ok        \n","10          None     ok        \n","11          None     ok        \n","12          None     ok        \n","13          None     ok        \n","14          None     ok        "],"text/html":["\n","  <div id=\"df-49b06e0c-33cd-45c2-b8e3-0735487e325c\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video</th>\n","      <th>csv2d</th>\n","      <th>csv3d</th>\n","      <th>annotated_mp4</th>\n","      <th>status</th>\n","      <th>error</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Aryeh_1_clip1_pose2d.csv</td>\n","      <td>outputs/Aryeh_1_clip1_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Aryeh_1_clip2_pose2d.csv</td>\n","      <td>outputs/Aryeh_1_clip2_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Aryeh_1_clip3_pose2d.csv</td>\n","      <td>outputs/Aryeh_1_clip3_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Aryeh_2_clip4_pose2d.csv</td>\n","      <td>outputs/Aryeh_2_clip4_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Aryeh_2_clip5_pose2d.csv</td>\n","      <td>outputs/Aryeh_2_clip5_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Danny_1_clip1_pose2d.csv</td>\n","      <td>outputs/Danny_1_clip1_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Danny_1_clip2_pose2d.csv</td>\n","      <td>outputs/Danny_1_clip2_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Danny_1_clip3_pose2d.csv</td>\n","      <td>outputs/Danny_1_clip3_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Danny_2_clip4_pose2d.csv</td>\n","      <td>outputs/Danny_2_clip4_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Danny_2_clip5_pose2d.csv</td>\n","      <td>outputs/Danny_2_clip5_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Max_1_clip1_pose2d.csv</td>\n","      <td>outputs/Max_1_clip1_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Max_1_clip2_pose2d.csv</td>\n","      <td>outputs/Max_1_clip2_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Max_1_clip3_pose2d.csv</td>\n","      <td>outputs/Max_1_clip3_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Max_2_clip4_pose2d.csv</td>\n","      <td>outputs/Max_2_clip4_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Max_2_clip5_pose2d.csv</td>\n","      <td>outputs/Max_2_clip5_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-49b06e0c-33cd-45c2-b8e3-0735487e325c')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-49b06e0c-33cd-45c2-b8e3-0735487e325c button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-49b06e0c-33cd-45c2-b8e3-0735487e325c');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-a6cb07bb-ffdc-4eef-8546-ca381736a8ac\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a6cb07bb-ffdc-4eef-8546-ca381736a8ac')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-a6cb07bb-ffdc-4eef-8546-ca381736a8ac button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","repr_error":"Out of range float values are not JSON compliant: nan"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["                                                video  \\\n","0   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","1   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","2   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","3   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","4   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","5   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","6   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","7   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","8   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","9   /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","10  /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","11  /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","12  /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","13  /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","14  /content/drive/MyDrive/Harvard/LS100/videos/Fr...   \n","\n","                               csv2d                             csv3d  \\\n","0   outputs/Aryeh_1_clip1_pose2d.csv  outputs/Aryeh_1_clip1_pose3d.csv   \n","1   outputs/Aryeh_1_clip2_pose2d.csv  outputs/Aryeh_1_clip2_pose3d.csv   \n","2   outputs/Aryeh_1_clip3_pose2d.csv  outputs/Aryeh_1_clip3_pose3d.csv   \n","3   outputs/Aryeh_2_clip4_pose2d.csv  outputs/Aryeh_2_clip4_pose3d.csv   \n","4   outputs/Aryeh_2_clip5_pose2d.csv  outputs/Aryeh_2_clip5_pose3d.csv   \n","5   outputs/Danny_1_clip1_pose2d.csv  outputs/Danny_1_clip1_pose3d.csv   \n","6   outputs/Danny_1_clip2_pose2d.csv  outputs/Danny_1_clip2_pose3d.csv   \n","7   outputs/Danny_1_clip3_pose2d.csv  outputs/Danny_1_clip3_pose3d.csv   \n","8   outputs/Danny_2_clip4_pose2d.csv  outputs/Danny_2_clip4_pose3d.csv   \n","9   outputs/Danny_2_clip5_pose2d.csv  outputs/Danny_2_clip5_pose3d.csv   \n","10    outputs/Max_1_clip1_pose2d.csv    outputs/Max_1_clip1_pose3d.csv   \n","11    outputs/Max_1_clip2_pose2d.csv    outputs/Max_1_clip2_pose3d.csv   \n","12    outputs/Max_1_clip3_pose2d.csv    outputs/Max_1_clip3_pose3d.csv   \n","13    outputs/Max_2_clip4_pose2d.csv    outputs/Max_2_clip4_pose3d.csv   \n","14    outputs/Max_2_clip5_pose2d.csv    outputs/Max_2_clip5_pose3d.csv   \n","\n","   annotated_mp4 status error  \n","0           None     ok        \n","1           None     ok        \n","2           None     ok        \n","3           None     ok        \n","4           None     ok        \n","5           None     ok        \n","6           None     ok        \n","7           None     ok        \n","8           None     ok        \n","9           None     ok        \n","10          None     ok        \n","11          None     ok        \n","12          None     ok        \n","13          None     ok        \n","14          None     ok        "],"text/html":["\n","  <div id=\"df-89dad142-0814-46b8-ae9f-311c9f37ea26\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video</th>\n","      <th>csv2d</th>\n","      <th>csv3d</th>\n","      <th>annotated_mp4</th>\n","      <th>status</th>\n","      <th>error</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Aryeh_1_clip1_pose2d.csv</td>\n","      <td>outputs/Aryeh_1_clip1_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Aryeh_1_clip2_pose2d.csv</td>\n","      <td>outputs/Aryeh_1_clip2_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Aryeh_1_clip3_pose2d.csv</td>\n","      <td>outputs/Aryeh_1_clip3_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Aryeh_2_clip4_pose2d.csv</td>\n","      <td>outputs/Aryeh_2_clip4_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Aryeh_2_clip5_pose2d.csv</td>\n","      <td>outputs/Aryeh_2_clip5_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Danny_1_clip1_pose2d.csv</td>\n","      <td>outputs/Danny_1_clip1_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Danny_1_clip2_pose2d.csv</td>\n","      <td>outputs/Danny_1_clip2_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Danny_1_clip3_pose2d.csv</td>\n","      <td>outputs/Danny_1_clip3_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Danny_2_clip4_pose2d.csv</td>\n","      <td>outputs/Danny_2_clip4_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Danny_2_clip5_pose2d.csv</td>\n","      <td>outputs/Danny_2_clip5_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Max_1_clip1_pose2d.csv</td>\n","      <td>outputs/Max_1_clip1_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Max_1_clip2_pose2d.csv</td>\n","      <td>outputs/Max_1_clip2_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Max_1_clip3_pose2d.csv</td>\n","      <td>outputs/Max_1_clip3_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Max_2_clip4_pose2d.csv</td>\n","      <td>outputs/Max_2_clip4_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>/content/drive/MyDrive/Harvard/LS100/videos/Fr...</td>\n","      <td>outputs/Max_2_clip5_pose2d.csv</td>\n","      <td>outputs/Max_2_clip5_pose3d.csv</td>\n","      <td>None</td>\n","      <td>ok</td>\n","      <td></td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89dad142-0814-46b8-ae9f-311c9f37ea26')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-89dad142-0814-46b8-ae9f-311c9f37ea26 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-89dad142-0814-46b8-ae9f-311c9f37ea26');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-0d0b68e1-07e6-4ba0-8b23-656ac26bdaea\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0d0b68e1-07e6-4ba0-8b23-656ac26bdaea')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-0d0b68e1-07e6-4ba0-8b23-656ac26bdaea button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_43938b87-3e99-4b28-8596-607c6293bbfe\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_43938b87-3e99-4b28-8596-607c6293bbfe button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","repr_error":"Out of range float values are not JSON compliant: nan"}},"metadata":{},"execution_count":18}],"source":["\n","#--- SINGLE VIDEO ---\n","#outputs = extract_pose_from_video(\n","#    video_path=\"/Users/zachbuller/Desktop/LS100/Trial_Videos/Danny_1.MP4\",\n","#    model_path=MODEL_PATH,\n","#    make_annotated_video=True,\n","#    frame_stride=2,   # increase to 2/3 for faster processing\n","# )\n","#print(outputs)\n","\n","#--- DIRECTORY ---\n","df = batch_process_videos(\n","    input_dir=\"/content/drive/MyDrive/Harvard/LS100/videos/Frame_Reduced/clips\",\n","    model_path=MODEL_PATH,\n","    out_dir=\"outputs\",\n","    make_annotated_video=False,\n","    frame_stride=2\n",")\n","\n","df\n","\n"]},{"cell_type":"markdown","id":"1ab198ad","metadata":{"id":"1ab198ad"},"source":["\n","---\n","\n","### Troubleshooting\n","- If you see `NoneType` for results, ensure the **model path exists** and your video actually contains a person.  \n","- If you get slowdowns or memory pressure, try `frame_stride=2` or the `\"lite\"` model.  \n","- On some platforms OpenCV MP4 writing may need codecs; if a saved video is empty, try a different `fourcc` (e.g., `cv2.VideoWriter_fourcc(*\"avc1\")`) or install `opencv-python-headless` alternatives.\n","\n","Happy exploring!\n"]},{"cell_type":"code","source":["!mv /content/outputs \"/content/drive/MyDrive/Harvard/LS100/videos/Frame_Reduced/clips/annotated_outputs\"\n"],"metadata":{"id":"DlnSo66f9GHC","executionInfo":{"status":"ok","timestamp":1763620458185,"user_tz":300,"elapsed":35232,"user":{"displayName":"Zachary Buller","userId":"10720990699951540462"}}},"id":"DlnSo66f9GHC","execution_count":21,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.12 (MediaPipe)","language":"python","name":"mediapipe_env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}